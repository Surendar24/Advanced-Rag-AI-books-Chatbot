# AI Book Chatbot with RAG Pipeline

An intelligent chatbot that answers questions about Artificial Intelligence using content from O'Reilly AI books, powered by Groq LLM and RAG (Retrieval-Augmented Generation).

## ğŸ“š Features

- **Accurate Citations**: Every answer includes inline citations in the format [Book Title, Chapter, Page]
- **Image Support**: Displays relevant diagrams and images from the books
- **RAG Pipeline**: Uses vector embeddings for semantic search and retrieval
- **Multi-Book Reasoning**: Combines content from multiple books for comprehensive answers
- **Modern UI**: React + TypeScript frontend with TanStack Query/Table
- **Fast API Backend**: Built with FastAPI for high performance

## ğŸ“– Books Included

1. AI Engineering
2. Applied Machine Learning and AI for Engineers
3. Hands-On Large Language Models
4. Hands-On Machine Learning with Scikit-Learn and PyTorch
5. LLM Engineers Handbook
6. NLP with Transformer Models

## ğŸ—ï¸ Project Structure

```
AI Book RAG/
â”œâ”€â”€ Books_pdf/                          # Source PDF files
â”œâ”€â”€ notebooks/                          # Jupyter notebooks for each process
â”‚   â”œâ”€â”€ 01_pdf_ingestion.ipynb         # Extract text, images, metadata
â”‚   â”œâ”€â”€ 02_text_chunking.ipynb         # Split and preprocess text
â”‚   â”œâ”€â”€ 03_embedding_vectordb.ipynb    # Generate embeddings, setup Chroma
â”‚   â””â”€â”€ 04_rag_pipeline_test.ipynb     # Test RAG pipeline
â”œâ”€â”€ backend/                            # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI app entry point
â”‚   â”‚   â”œâ”€â”€ models.py                  # Pydantic models
â”‚   â”‚   â”œâ”€â”€ rag_engine.py              # RAG pipeline logic
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ chat.py                # Chat endpoints
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/                           # React + TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/                # React components
â”‚   â”‚   â”œâ”€â”€ hooks/                     # TanStack Query hooks
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ data/                               # Processed data
â”‚   â”œâ”€â”€ extracted/                     # Extracted text and images
â”‚   â””â”€â”€ chunks/                        # Chunked text with metadata
â”œâ”€â”€ chroma_db/                         # Vector database storage
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .env.example                       # Environment variables template
â””â”€â”€ README.md
```

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.10+
- Node.js 18+
- Groq API Key (free tier available at https://console.groq.com)

### Backend Setup

1. **Clone and navigate to the project**:
   ```bash
   cd "d:\AI Book RAG"
   ```

2. **Create virtual environment**:
   ```bash
   python -m venv venv
   venv\Scripts\activate  # Windows
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**:
   ```bash
   copy .env.example .env
   # Edit .env and add your GROQ_API_KEY
   ```

5. **Run the notebooks in order**:
   - `01_pdf_ingestion.ipynb` - Extract content from PDFs
   - `02_text_chunking.ipynb` - Chunk and preprocess text
   - `03_embedding_vectordb.ipynb` - Generate embeddings and populate vector DB
   - `04_rag_pipeline_test.ipynb` - Test the RAG pipeline

6. **Start the backend**:
   ```bash
   cd backend
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

### Frontend Setup

1. **Navigate to frontend**:
   ```bash
   cd frontend
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Start development server**:
   ```bash
   npm run dev
   ```

4. **Open browser**:
   ```
   http://localhost:3000
   ```

## ğŸ“ Usage

1. Open the web interface
2. Type your AI-related question in the chat input
3. Receive answers with:
   - Inline citations [Book Title, Chapter, Page]
   - Relevant diagrams/images
   - Source snippets (optional)

### Example Questions

- "What is transfer learning and how does it work?"
- "Explain the transformer architecture"
- "What are the best practices for fine-tuning LLMs?"
- "How do I implement a neural network with PyTorch?"

## ğŸ”§ Configuration

### RAG Parameters (in `.env`)

- `TOP_K_RESULTS`: Number of relevant chunks to retrieve (default: 5)
- `CHUNK_SIZE`: Size of text chunks in characters (default: 1000)
- `CHUNK_OVERLAP`: Overlap between chunks (default: 200)
- `TEMPERATURE`: LLM temperature for response generation (default: 0.1)

### Embedding Model

Default: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions, fast)

Alternatives:
- `sentence-transformers/all-mpnet-base-v2` (768 dimensions, more accurate)
- `BAAI/bge-small-en-v1.5` (384 dimensions, optimized for retrieval)

## ğŸš¢ Deployment

### Backend (Render/Railway)

1. Push code to GitHub
2. Connect repository to Render/Railway
3. Set environment variables
4. Deploy with auto-build

### Frontend (Vercel)

1. Push frontend code to GitHub
2. Import project to Vercel
3. Configure build settings:
   - Build Command: `npm run build`
   - Output Directory: `dist`
4. Deploy

### Vector Database

- **Development**: Local Chroma DB
- **Production**: Consider Qdrant Cloud (free tier) or persist Chroma to volume

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework**: FastAPI
- **LLM**: Groq (Mixtral-8x7b)
- **Embeddings**: Sentence Transformers
- **Vector DB**: ChromaDB
- **PDF Processing**: PyMuPDF, PDFPlumber

### Frontend
- **Framework**: React + TypeScript
- **State Management**: TanStack Query
- **Table Display**: TanStack Table
- **Styling**: Tailwind CSS
- **Icons**: Lucide React

## ğŸ“Š Advanced Features

- âœ… Multi-book reasoning
- âœ… Inline citations with metadata
- âœ… Image/diagram extraction and display
- âœ… Source snippet viewing
- âœ… Query caching with TanStack Query
- âœ… Confidence scoring for citations
- âœ… Automatic diagram highlighting

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is for educational purposes. Please respect the copyright of the O'Reilly books.

## ğŸ™ Acknowledgments

- O'Reilly Media for the excellent AI books
- Groq for providing fast LLM inference
- ChromaDB for the vector database
- TanStack for Query and Table libraries

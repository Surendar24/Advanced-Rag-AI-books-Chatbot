{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: PDF Ingestion\n",
    "\n",
    "## Purpose\n",
    "This notebook extracts text, images, and metadata from O'Reilly AI books in PDF format.\n",
    "\n",
    "## Process\n",
    "1. Load all PDFs from the Books_pdf directory\n",
    "2. Extract text content page by page\n",
    "3. Extract images and diagrams\n",
    "4. Extract metadata (book title, chapter, page numbers)\n",
    "5. Save extracted data in structured format\n",
    "\n",
    "## Output\n",
    "- Extracted text with metadata (JSON format)\n",
    "- Extracted images saved to data/images/\n",
    "- Summary statistics of extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Directory: d:\\AI Book RAG\\Books_pdf\n",
      "Output Directory: d:\\AI Book RAG\\data\\extracted\n",
      "Image Directory: d:\\AI Book RAG\\data\\images\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Set up paths and directories\n",
    "BASE_DIR = Path(r\"d:\\AI Book RAG\")\n",
    "PDF_DIR = BASE_DIR / \"Books_pdf\"\n",
    "OUTPUT_DIR = BASE_DIR / \"data\" / \"extracted\"\n",
    "IMAGE_DIR = BASE_DIR / \"data\" / \"images\"\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PDF Directory: {PDF_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Image Directory: {IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Extract book title from filename\n",
    "def extract_book_title(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract a clean book title from the PDF filename.\n",
    "    Removes file extension and common suffixes like (Z-Library).\n",
    "    \"\"\"\n",
    "    # Remove .pdf extension\n",
    "    title = filename.replace('.pdf', '')\n",
    "    \n",
    "    # Remove common suffixes\n",
    "    title = re.sub(r'\\s*\\([^)]*Z-Library[^)]*\\)', '', title)\n",
    "    title = re.sub(r'\\s*\\([^)]*\\)\\s*$', '', title)\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    title = ' '.join(title.split())\n",
    "    \n",
    "    return title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Detect chapter from page content\n",
    "def detect_chapter(text: str, prev_chapter: str = \"Introduction\") -> str:\n",
    "    \"\"\"\n",
    "    Attempt to detect chapter title from page text.\n",
    "    Looks for common patterns like \"Chapter X\" or \"CHAPTER X:\".\n",
    "    \"\"\"\n",
    "    # Pattern 1: \"Chapter X: Title\" or \"CHAPTER X: Title\"\n",
    "    chapter_pattern1 = re.search(r'(?:Chapter|CHAPTER)\\s+(\\d+|[IVX]+)\\s*[:\\-]?\\s*([^\\n]{0,100})', text[:500])\n",
    "    if chapter_pattern1:\n",
    "        chapter_num = chapter_pattern1.group(1)\n",
    "        chapter_title = chapter_pattern1.group(2).strip()\n",
    "        return f\"Chapter {chapter_num}: {chapter_title}\" if chapter_title else f\"Chapter {chapter_num}\"\n",
    "    \n",
    "    # Pattern 2: Look for numbered sections\n",
    "    section_pattern = re.search(r'^(\\d+\\.\\d+|\\d+)\\s+([A-Z][^\\n]{10,80})$', text[:500], re.MULTILINE)\n",
    "    if section_pattern:\n",
    "        return section_pattern.group(0).strip()\n",
    "    \n",
    "    # Default: return previous chapter\n",
    "    return prev_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Generate unique image ID\n",
    "def generate_image_id(book_title: str, page_num: int, img_index: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for an image.\n",
    "    Format: hash of book_title + page number + image index\n",
    "    \"\"\"\n",
    "    book_hash = hashlib.md5(book_title.encode()).hexdigest()[:8]\n",
    "    return f\"{book_hash}_p{page_num}_img{img_index}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Extract text and metadata from a single PDF\n",
    "def extract_pdf_content(pdf_path: Path, book_title: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract text content and metadata from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        book_title: Title of the book\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing extracted pages with text and metadata\n",
    "    \"\"\"\n",
    "    pages_data = []\n",
    "    current_chapter = \"Introduction\"\n",
    "    \n",
    "    # Open PDF with pdfplumber for text extraction\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        \n",
    "        print(f\"\\nProcessing: {book_title}\")\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "        \n",
    "        # Iterate through each page\n",
    "        for page_num, page in enumerate(tqdm(pdf.pages, desc=\"Extracting text\"), start=1):\n",
    "            # Extract text from page\n",
    "            text = page.extract_text() or \"\"\n",
    "            \n",
    "            # Skip empty pages\n",
    "            if len(text.strip()) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Detect chapter (update if new chapter found)\n",
    "            detected_chapter = detect_chapter(text, current_chapter)\n",
    "            if detected_chapter != current_chapter and \"Chapter\" in detected_chapter:\n",
    "                current_chapter = detected_chapter\n",
    "            \n",
    "            # Store page data\n",
    "            page_data = {\n",
    "                \"book_title\": book_title,\n",
    "                \"chapter\": current_chapter,\n",
    "                \"page_number\": page_num,\n",
    "                \"text\": text,\n",
    "                \"char_count\": len(text),\n",
    "                \"word_count\": len(text.split())\n",
    "            }\n",
    "            \n",
    "            pages_data.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        \"book_title\": book_title,\n",
    "        \"total_pages\": len(pages_data),\n",
    "        \"pages\": pages_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Extract images from a single PDF\n",
    "def extract_pdf_images(pdf_path: Path, book_title: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract images and diagrams from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        book_title: Title of the book\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing image metadata\n",
    "    \"\"\"\n",
    "    images_data = []\n",
    "    \n",
    "    # Open PDF with PyMuPDF for image extraction\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    print(f\"\\nExtracting images from: {book_title}\")\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_num in tqdm(range(len(pdf_document)), desc=\"Extracting images\"):\n",
    "        page = pdf_document[page_num]\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        # Extract each image from the page\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            \n",
    "            # Get image data\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            \n",
    "            # Generate unique image ID and filename\n",
    "            image_id = generate_image_id(book_title, page_num + 1, img_index)\n",
    "            image_filename = f\"{image_id}.{image_ext}\"\n",
    "            image_path = IMAGE_DIR / image_filename\n",
    "            \n",
    "            # Save image to disk\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            \n",
    "            # Store image metadata\n",
    "            image_data = {\n",
    "                \"image_id\": image_id,\n",
    "                \"book_title\": book_title,\n",
    "                \"page_number\": page_num + 1,\n",
    "                \"image_index\": img_index,\n",
    "                \"filename\": image_filename,\n",
    "                \"path\": str(image_path),\n",
    "                \"format\": image_ext\n",
    "            }\n",
    "            \n",
    "            images_data.append(image_data)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    \n",
    "    return images_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 PDF files to process\n",
      "\n",
      "\n",
      "Processing: AI Engineering\n",
      "Total pages: 535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|██████████| 535/535 [01:04<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting images from: AI Engineering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 535/535 [00:13<00:00, 38.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: AI Engineering\n",
      "  - Pages extracted: 529\n",
      "  - Images extracted: 224\n",
      "\n",
      "Processing: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "Total pages: 666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|██████████| 666/666 [01:13<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting images from: Applied-Machine-Learning-and-AI-for-Engineers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 666/666 [00:00<00:00, 952.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "  - Pages extracted: 661\n",
      "  - Images extracted: 227\n",
      "\n",
      "Processing: Hands-On Large Language Models Language Understanding and Generation\n",
      "Total pages: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|██████████| 428/428 [00:45<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting images from: Hands-On Large Language Models Language Understanding and Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 428/428 [00:16<00:00, 25.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: Hands-On Large Language Models Language Understanding and Generation\n",
      "  - Pages extracted: 413\n",
      "  - Images extracted: 329\n",
      "\n",
      "Processing: Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release)\n",
      "Total pages: 608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|██████████| 608/608 [02:50<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting images from: Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 608/608 [00:30<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release)\n",
      "  - Pages extracted: 532\n",
      "  - Images extracted: 168\n",
      "\n",
      "Processing: LLM Engineers Handbook\n",
      "Total pages: 523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|██████████| 523/523 [02:07<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting images from: LLM Engineers Handbook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 523/523 [00:04<00:00, 110.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: LLM Engineers Handbook\n",
      "  - Pages extracted: 513\n",
      "  - Images extracted: 116\n",
      "\n",
      "Processing: NLP with Transformer models\n",
      "Total pages: 409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|██████████| 409/409 [02:01<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting images from: NLP with Transformer models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 409/409 [00:26<00:00, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: NLP with Transformer models\n",
      "  - Pages extracted: 403\n",
      "  - Images extracted: 272\n",
      "\n",
      "============================================================\n",
      "Extraction complete!\n",
      "Total books processed: 6\n",
      "Total images extracted: 1336\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in the Books_pdf directory\n",
    "all_books_data = []\n",
    "all_images_data = []\n",
    "\n",
    "# Get list of PDF files\n",
    "pdf_files = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files to process\\n\")\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    # Extract book title from filename\n",
    "    book_title = extract_book_title(pdf_file.name)\n",
    "    \n",
    "    try:\n",
    "        # Extract text content\n",
    "        book_data = extract_pdf_content(pdf_file, book_title)\n",
    "        all_books_data.append(book_data)\n",
    "        \n",
    "        # Extract images\n",
    "        images_data = extract_pdf_images(pdf_file, book_title)\n",
    "        all_images_data.extend(images_data)\n",
    "        \n",
    "        print(f\"✓ Completed: {book_title}\")\n",
    "        print(f\"  - Pages extracted: {book_data['total_pages']}\")\n",
    "        print(f\"  - Images extracted: {len(images_data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {book_title}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Extraction complete!\")\n",
    "print(f\"Total books processed: {len(all_books_data)}\")\n",
    "print(f\"Total images extracted: {len(all_images_data)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving extracted data...\n",
      "✓ Saved: AI_Engineering.json\n",
      "✓ Saved: AppliedMachineLearningandAIforEngineers.json\n",
      "✓ Saved: HandsOn_Large_Language_Models_Language_Understanding_and_Generation.json\n",
      "✓ Saved: HandsOn_Machine_Learning_with_ScikitLearn_and_PyTorch_Second_Early_Release.json\n",
      "✓ Saved: LLM_Engineers_Handbook.json\n",
      "✓ Saved: NLP_with_Transformer_models.json\n",
      "✓ Saved: all_books_combined.json\n",
      "✓ Saved: images_metadata.json\n",
      "\n",
      "All data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save extracted text data to JSON files\n",
    "print(\"\\nSaving extracted data...\")\n",
    "\n",
    "# Save each book's data separately\n",
    "for book_data in all_books_data:\n",
    "    book_title_safe = re.sub(r'[^a-zA-Z0-9\\s]', '', book_data['book_title'])\n",
    "    book_title_safe = '_'.join(book_title_safe.split())\n",
    "    \n",
    "    output_file = OUTPUT_DIR / f\"{book_title_safe}.json\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(book_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Saved: {output_file.name}\")\n",
    "\n",
    "# Save combined data\n",
    "combined_file = OUTPUT_DIR / \"all_books_combined.json\"\n",
    "with open(combined_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_books_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved: {combined_file.name}\")\n",
    "\n",
    "# Save images metadata\n",
    "images_file = OUTPUT_DIR / \"images_metadata.json\"\n",
    "with open(images_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_images_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved: {images_file.name}\")\n",
    "\n",
    "print(\"\\nAll data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "                                                                    Book Title  Pages Total Characters Total Words  Images\n",
      "                                                                AI Engineering    529        1,080,290     173,374     224\n",
      "                                 Applied-Machine-Learning-and-AI-for-Engineers    661          730,604     108,504     227\n",
      "          Hands-On Large Language Models Language Understanding and Generation    413          623,488      95,614     329\n",
      "Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release)    532          825,706     131,905     168\n",
      "                                                        LLM Engineers Handbook    513          869,917     128,613     116\n",
      "                                                   NLP with Transformer models    403          747,135     114,067     272\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display extraction statistics\n",
    "import pandas as pd\n",
    "\n",
    "# Create summary statistics\n",
    "stats = []\n",
    "for book_data in all_books_data:\n",
    "    total_chars = sum(page['char_count'] for page in book_data['pages'])\n",
    "    total_words = sum(page['word_count'] for page in book_data['pages'])\n",
    "    \n",
    "    book_images = [img for img in all_images_data if img['book_title'] == book_data['book_title']]\n",
    "    \n",
    "    stats.append({\n",
    "        'Book Title': book_data['book_title'],\n",
    "        'Pages': book_data['total_pages'],\n",
    "        'Total Characters': f\"{total_chars:,}\",\n",
    "        'Total Words': f\"{total_words:,}\",\n",
    "        'Images': len(book_images)\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df_stats = pd.DataFrame(stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_stats.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE: First Page of First Book\n",
      "================================================================================\n",
      "Book: AI Engineering\n",
      "Chapter: Introduction\n",
      "Page: 1\n",
      "\n",
      "Text Preview (first 500 characters):\n",
      "--------------------------------------------------------------------------------\n",
      "AI Engineering\n",
      "Building Applications\n",
      "with Foundation Models\n",
      "Chip Huyen\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sample: Display first page of first book\n",
    "if all_books_data:\n",
    "    first_book = all_books_data[0]\n",
    "    first_page = first_book['pages'][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE: First Page of First Book\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Book: {first_page['book_title']}\")\n",
    "    print(f\"Chapter: {first_page['chapter']}\")\n",
    "    print(f\"Page: {first_page['page_number']}\")\n",
    "    print(f\"\\nText Preview (first 500 characters):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(first_page['text'][:500])\n",
    "    print(\"...\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "✅ PDF ingestion complete!\n",
    "\n",
    "The extracted data is now ready for the next stage:\n",
    "- **Notebook 2**: Text chunking and preprocessing\n",
    "\n",
    "### Output Files Created:\n",
    "1. `data/extracted/[BookTitle].json` - Individual book data\n",
    "2. `data/extracted/all_books_combined.json` - Combined data from all books\n",
    "3. `data/extracted/images_metadata.json` - Metadata for all extracted images\n",
    "4. `data/images/` - Directory containing all extracted images\n",
    "\n",
    "### What's Next:\n",
    "In the next notebook, we will:\n",
    "1. Load the extracted text data\n",
    "2. Split text into semantic chunks\n",
    "3. Add metadata to each chunk (book, chapter, page)\n",
    "4. Prepare chunks for embedding generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_RAG",
   "language": "python",
   "name": "ai_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

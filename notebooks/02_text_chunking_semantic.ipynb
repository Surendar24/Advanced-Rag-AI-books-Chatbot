{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Semantic Text Chunking\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook implements **semantic chunking** - an advanced RAG technique that creates chunks based on meaning and context rather than arbitrary character counts.\n",
    "\n",
    "## Semantic Chunking Benefits\n",
    "\n",
    "1. **Meaning-based boundaries** - Chunks break at natural semantic boundaries\n",
    "2. **Better retrieval** - More coherent chunks improve embedding quality\n",
    "3. **Context preservation** - Maintains topic coherence within chunks\n",
    "4. **Reduced fragmentation** - Avoids splitting related concepts\n",
    "\n",
    "## Process\n",
    "\n",
    "1. Load extracted text data from Notebook 1\n",
    "2. Use sentence embeddings to detect semantic boundaries\n",
    "3. Group sentences with similar embeddings into chunks\n",
    "4. Add context enrichment (surrounding context)\n",
    "5. Preserve metadata (book, chapter, page)\n",
    "6. Save semantic chunks for embedding generation\n",
    "\n",
    "## Output\n",
    "\n",
    "- Semantically coherent chunks with metadata\n",
    "- Context-enriched chunks for better retrieval\n",
    "- Ready for embedding generation in Notebook 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jagth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jagth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data Directory: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\data\\extracted\n",
      "Chunks Output Directory: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\data\\semantic_chunks\n",
      "\n",
      "Semantic Chunking Configuration:\n",
      "  - Similarity Threshold: 0.75\n",
      "  - Chunk Size: 3-10 sentences\n",
      "  - Character Range: 200-1500 chars\n",
      "  - Context Enrichment: ±1 sentences\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(r\"c:\\\\Users\\\\jagth\\\\Downloads\\\\New folder\\\\ai-books-rag-chatbot\\\\Books_pdf\")\n",
    "EXTRACTED_DIR = BASE_DIR / \"data\" / \"extracted\"\n",
    "CHUNKS_DIR = BASE_DIR / \"data\" / \"semantic_chunks\"\n",
    "\n",
    "# Create output directory\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Semantic chunking parameters\n",
    "SIMILARITY_THRESHOLD = 0.75  # Cosine similarity threshold for grouping sentences\n",
    "MIN_CHUNK_SENTENCES = 3      # Minimum sentences per chunk\n",
    "MAX_CHUNK_SENTENCES = 10     # Maximum sentences per chunk\n",
    "MIN_CHUNK_CHARS = 200        # Minimum characters per chunk\n",
    "MAX_CHUNK_CHARS = 1500       # Maximum characters per chunk\n",
    "CONTEXT_SENTENCES = 1        # Number of sentences to add as context before/after\n",
    "\n",
    "print(f\"Extracted Data Directory: {EXTRACTED_DIR}\")\n",
    "print(f\"Chunks Output Directory: {CHUNKS_DIR}\")\n",
    "print(f\"\\nSemantic Chunking Configuration:\")\n",
    "print(f\"  - Similarity Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  - Chunk Size: {MIN_CHUNK_SENTENCES}-{MAX_CHUNK_SENTENCES} sentences\")\n",
    "print(f\"  - Character Range: {MIN_CHUNK_CHARS}-{MAX_CHUNK_CHARS} chars\")\n",
    "print(f\"  - Context Enrichment: ±{CONTEXT_SENTENCES} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence embedding model...\n",
      "✓ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model for semantic similarity\n",
    "print(\"Loading sentence embedding model...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extracted data from: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\data\\extracted\\all_books_combined.json\n",
      "✓ Loaded data for 6 books\n",
      "✓ Total pages to process: 3051\n"
     ]
    }
   ],
   "source": [
    "# Load extracted data\n",
    "combined_file = EXTRACTED_DIR / \"all_books_combined.json\"\n",
    "print(f\"Loading extracted data from: {combined_file}\")\n",
    "\n",
    "with open(combined_file, 'r', encoding='utf-8') as f:\n",
    "    all_books_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded data for {len(all_books_data)} books\")\n",
    "total_pages = sum(book['total_pages'] for book in all_books_data)\n",
    "print(f\"✓ Total pages to process: {total_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Clean text\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    # Replace multiple newlines with double newline\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    # Remove leading/trailing whitespace from each line\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "    # Remove standalone page numbers\n",
    "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    # Clean up again\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Helper: Generate chunk ID\n",
    "def generate_chunk_id(book_title: str, page_num: int, chunk_index: int) -> str:\n",
    "    \"\"\"Generate unique chunk identifier\"\"\"\n",
    "    book_hash = hashlib.md5(book_title.encode()).hexdigest()[:8]\n",
    "    return f\"{book_hash}_p{page_num}_sc{chunk_index}\"\n",
    "\n",
    "# Helper: Calculate semantic similarity between sentences\n",
    "def calculate_sentence_similarities(sentences: List[str], embeddings: np.ndarray) -> List[float]:\n",
    "    \"\"\"Calculate cosine similarity between consecutive sentences\"\"\"\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        sim = cosine_similarity(\n",
    "            embeddings[i].reshape(1, -1),\n",
    "            embeddings[i + 1].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append(sim)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core: Semantic chunking function\n",
    "def semantic_chunk_text(\n",
    "    text: str,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD,\n",
    "    min_sentences: int = MIN_CHUNK_SENTENCES,\n",
    "    max_sentences: int = MAX_CHUNK_SENTENCES,\n",
    "    min_chars: int = MIN_CHUNK_CHARS,\n",
    "    max_chars: int = MAX_CHUNK_CHARS\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into semantic chunks based on sentence similarity.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Split text into sentences\n",
    "    2. Generate embeddings for each sentence\n",
    "    3. Calculate similarity between consecutive sentences\n",
    "    4. Group sentences where similarity > threshold\n",
    "    5. Respect min/max constraints\n",
    "    \"\"\"\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    if len(sentences) < min_sentences:\n",
    "        return [text] if len(text) >= min_chars else []\n",
    "    \n",
    "    # Generate embeddings for all sentences\n",
    "    embeddings = embedding_model.encode(sentences, show_progress_bar=False)\n",
    "    \n",
    "    # Calculate similarities between consecutive sentences\n",
    "    similarities = calculate_sentence_similarities(sentences, embeddings)\n",
    "    \n",
    "    # Find semantic boundaries (where similarity drops below threshold)\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_chars = len(sentences[0])\n",
    "    \n",
    "    for i, (sentence, similarity) in enumerate(zip(sentences[1:], similarities), 1):\n",
    "        sentence_len = len(sentence)\n",
    "        \n",
    "        # Check if we should start a new chunk\n",
    "        should_break = (\n",
    "            # Semantic boundary detected\n",
    "            similarity < similarity_threshold or\n",
    "            # Max sentences reached\n",
    "            len(current_chunk) >= max_sentences or\n",
    "            # Max chars would be exceeded\n",
    "            (current_chars + sentence_len > max_chars and len(current_chunk) >= min_sentences)\n",
    "        )\n",
    "        \n",
    "        if should_break and len(current_chunk) >= min_sentences and current_chars >= min_chars:\n",
    "            # Save current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chars = sentence_len\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_chars += sentence_len\n",
    "    \n",
    "    # Add final chunk if it meets criteria\n",
    "    if current_chunk and len(current_chunk) >= min_sentences and current_chars >= min_chars:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    elif current_chunk and chunks:  # Merge small final chunk with previous\n",
    "        chunks[-1] += ' ' + ' '.join(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context enrichment: Add surrounding context to chunks\n",
    "def enrich_chunk_with_context(\n",
    "    chunk_text: str,\n",
    "    page_text: str,\n",
    "    context_sentences: int = CONTEXT_SENTENCES\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Add context from surrounding sentences to improve retrieval.\n",
    "    Returns dict with main chunk and context.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(page_text)\n",
    "    chunk_sentences = sent_tokenize(chunk_text)\n",
    "    \n",
    "    if not chunk_sentences:\n",
    "        return {'main': chunk_text, 'context': ''}\n",
    "    \n",
    "    # Find where chunk starts in page\n",
    "    first_chunk_sent = chunk_sentences[0]\n",
    "    try:\n",
    "        start_idx = sentences.index(first_chunk_sent)\n",
    "    except ValueError:\n",
    "        return {'main': chunk_text, 'context': ''}\n",
    "    \n",
    "    # Get context before\n",
    "    context_before = []\n",
    "    for i in range(max(0, start_idx - context_sentences), start_idx):\n",
    "        context_before.append(sentences[i])\n",
    "    \n",
    "    # Get context after\n",
    "    end_idx = start_idx + len(chunk_sentences)\n",
    "    context_after = []\n",
    "    for i in range(end_idx, min(len(sentences), end_idx + context_sentences)):\n",
    "        context_after.append(sentences[i])\n",
    "    \n",
    "    context = ' '.join(context_before + context_after)\n",
    "    \n",
    "    return {\n",
    "        'main': chunk_text,\n",
    "        'context': context\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main: Process all books with semantic chunking\n",
    "def create_semantic_chunks_from_books(books_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process all books and create semantic chunks with context enrichment.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    chunk_global_index = 0\n",
    "    \n",
    "    for book_data in books_data:\n",
    "        book_title = book_data['book_title']\n",
    "        pages = book_data['pages']\n",
    "        \n",
    "        print(f\"\\nProcessing: {book_title}\")\n",
    "        print(f\"Pages: {len(pages)}\")\n",
    "        \n",
    "        book_chunks = []\n",
    "        \n",
    "        for page_data in tqdm(pages, desc=f\"Semantic chunking {book_title}\"):\n",
    "            # Clean text\n",
    "            cleaned_text = clean_text(page_data['text'])\n",
    "            \n",
    "            if len(cleaned_text) < MIN_CHUNK_CHARS:\n",
    "                continue\n",
    "            \n",
    "            # Create semantic chunks\n",
    "            text_chunks = semantic_chunk_text(\n",
    "                cleaned_text,\n",
    "                embedding_model\n",
    "            )\n",
    "            \n",
    "            # Process each chunk\n",
    "            for chunk_index, chunk_text in enumerate(text_chunks):\n",
    "                # Add context enrichment\n",
    "                enriched = enrich_chunk_with_context(chunk_text, cleaned_text)\n",
    "                \n",
    "                # Combine main chunk with context for embedding\n",
    "                full_text = chunk_text\n",
    "                if enriched['context']:\n",
    "                    full_text = f\"{enriched['context']} {chunk_text}\"\n",
    "                \n",
    "                # Generate chunk ID\n",
    "                chunk_id = generate_chunk_id(\n",
    "                    book_title,\n",
    "                    page_data['page_number'],\n",
    "                    chunk_index\n",
    "                )\n",
    "                \n",
    "                # Create chunk object\n",
    "                chunk = {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"global_index\": chunk_global_index,\n",
    "                    \"book_title\": book_title,\n",
    "                    \"chapter\": page_data['chapter'],\n",
    "                    \"page_number\": page_data['page_number'],\n",
    "                    \"chunk_index\": chunk_index,\n",
    "                    \"text\": chunk_text,  # Main chunk for display\n",
    "                    \"text_with_context\": full_text,  # For embedding\n",
    "                    \"context\": enriched['context'],\n",
    "                    \"char_count\": len(chunk_text),\n",
    "                    \"word_count\": len(chunk_text.split()),\n",
    "                    \"sentence_count\": len(sent_tokenize(chunk_text)),\n",
    "                    \"citation\": f\"[{book_title}, {page_data['chapter']}, Page {page_data['page_number']}]\",\n",
    "                    \"chunking_method\": \"semantic\"\n",
    "                }\n",
    "                \n",
    "                book_chunks.append(chunk)\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_global_index += 1\n",
    "        \n",
    "        print(f\"✓ Created {len(book_chunks)} semantic chunks from {book_title}\")\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting semantic chunking process...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Processing: AI Engineering\n",
      "Pages: 529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking AI Engineering: 100%|██████████| 529/529 [01:34<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 2938 semantic chunks from AI Engineering\n",
      "\n",
      "Processing: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "Pages: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking Applied-Machine-Learning-and-AI-for-Engineers: 100%|██████████| 661/661 [01:04<00:00, 10.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1422 semantic chunks from Applied-Machine-Learning-and-AI-for-Engineers\n",
      "\n",
      "Processing: Hands-On Large Language Models Language Understanding and Generation\n",
      "Pages: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking Hands-On Large Language Models Language Understanding and Generation: 100%|██████████| 413/413 [01:21<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1394 semantic chunks from Hands-On Large Language Models Language Understanding and Generation\n",
      "\n",
      "Processing: Hands-On Machine Learning with Scikit-Learn and PyTorch\n",
      "Pages: 532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking Hands-On Machine Learning with Scikit-Learn and PyTorch: 100%|██████████| 532/532 [01:47<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1726 semantic chunks from Hands-On Machine Learning with Scikit-Learn and PyTorch\n",
      "\n",
      "Processing: LLM Engineers Handbook\n",
      "Pages: 513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking LLM Engineers Handbook: 100%|██████████| 513/513 [01:25<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1862 semantic chunks from LLM Engineers Handbook\n",
      "\n",
      "Processing: NLP with Transformer models\n",
      "Pages: 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking NLP with Transformer models: 100%|██████████| 403/403 [01:40<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1278 semantic chunks from NLP with Transformer models\n",
      "\n",
      "================================================================================\n",
      "Semantic chunking complete!\n",
      "Total chunks created: 10620\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute semantic chunking\n",
    "print(\"Starting semantic chunking process...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_chunks = create_semantic_chunks_from_books(all_books_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Semantic chunking complete!\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving semantic chunks...\n",
      "✓ Saved: all_chunks_semantic.json\n",
      "✓ Saved: AI_Engineering_chunks_semantic.json (2938 chunks)\n",
      "✓ Saved: AppliedMachineLearningandAIforEngineers_chunks_semantic.json (1422 chunks)\n",
      "✓ Saved: HandsOn_Large_Language_Models_Language_Understanding_and_Generation_chunks_semantic.json (1394 chunks)\n",
      "✓ Saved: HandsOn_Machine_Learning_with_ScikitLearn_and_PyTorch_chunks_semantic.json (1726 chunks)\n",
      "✓ Saved: LLM_Engineers_Handbook_chunks_semantic.json (1862 chunks)\n",
      "✓ Saved: NLP_with_Transformer_models_chunks_semantic.json (1278 chunks)\n",
      "\n",
      "All semantic chunks saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save semantic chunks\n",
    "print(\"\\nSaving semantic chunks...\")\n",
    "\n",
    "# Save all chunks\n",
    "chunks_file = CHUNKS_DIR / \"all_chunks_semantic.json\"\n",
    "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved: {chunks_file.name}\")\n",
    "\n",
    "# Save by book\n",
    "chunks_by_book = {}\n",
    "for chunk in all_chunks:\n",
    "    book_title = chunk['book_title']\n",
    "    if book_title not in chunks_by_book:\n",
    "        chunks_by_book[book_title] = []\n",
    "    chunks_by_book[book_title].append(chunk)\n",
    "\n",
    "for book_title, chunks in chunks_by_book.items():\n",
    "    book_title_safe = re.sub(r'[^a-zA-Z0-9\\s]', '', book_title)\n",
    "    book_title_safe = '_'.join(book_title_safe.split())\n",
    "    \n",
    "    book_chunks_file = CHUNKS_DIR / f\"{book_title_safe}_chunks_semantic.json\"\n",
    "    with open(book_chunks_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Saved: {book_chunks_file.name} ({len(chunks)} chunks)\")\n",
    "\n",
    "print(\"\\nAll semantic chunks saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SEMANTIC CHUNKING STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total Chunks: 10,620\n",
      "  Total Characters: 4,861,148\n",
      "  Total Words: 750,403\n",
      "  Total Sentences: 37,969\n",
      "\n",
      "Average per Chunk:\n",
      "  Characters: 457.7\n",
      "  Words: 70.7\n",
      "  Sentences: 3.6\n",
      "\n",
      "Context Enrichment:\n",
      "  Chunks with context: 9991 (94.1%)\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC CHUNKING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_chunks = len(all_chunks)\n",
    "total_chars = sum(chunk['char_count'] for chunk in all_chunks)\n",
    "total_words = sum(chunk['word_count'] for chunk in all_chunks)\n",
    "total_sentences = sum(chunk['sentence_count'] for chunk in all_chunks)\n",
    "\n",
    "avg_chars = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "avg_words = total_words / total_chunks if total_chunks > 0 else 0\n",
    "avg_sentences = total_sentences / total_chunks if total_chunks > 0 else 0\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"  Total Chunks: {total_chunks:,}\")\n",
    "print(f\"  Total Characters: {total_chars:,}\")\n",
    "print(f\"  Total Words: {total_words:,}\")\n",
    "print(f\"  Total Sentences: {total_sentences:,}\")\n",
    "print(f\"\\nAverage per Chunk:\")\n",
    "print(f\"  Characters: {avg_chars:.1f}\")\n",
    "print(f\"  Words: {avg_words:.1f}\")\n",
    "print(f\"  Sentences: {avg_sentences:.1f}\")\n",
    "\n",
    "# Chunks with context enrichment\n",
    "chunks_with_context = sum(1 for chunk in all_chunks if chunk['context'])\n",
    "print(f\"\\nContext Enrichment:\")\n",
    "print(f\"  Chunks with context: {chunks_with_context} ({chunks_with_context/total_chunks*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STATISTICS BY BOOK\n",
      "================================================================================\n",
      "                                                          Book Title  Chunks Avg Sentences Avg Words Avg Chars\n",
      "                                                      AI Engineering    2938           3.6        59       367\n",
      "                       Applied-Machine-Learning-and-AI-for-Engineers    1422           3.2        76       511\n",
      "Hands-On Large Language Models Language Understanding and Generation    1394           3.9        69       446\n",
      "             Hands-On Machine Learning with Scikit-Learn and PyTorch    1726           3.4        76       476\n",
      "                                              LLM Engineers Handbook    1862           3.4        69       466\n",
      "                                         NLP with Transformer models    1278           3.9        89       584\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Statistics by book\n",
    "book_stats = []\n",
    "for book_title, chunks in chunks_by_book.items():\n",
    "    book_chars = sum(chunk['char_count'] for chunk in chunks)\n",
    "    book_words = sum(chunk['word_count'] for chunk in chunks)\n",
    "    book_sentences = sum(chunk['sentence_count'] for chunk in chunks)\n",
    "    \n",
    "    book_stats.append({\n",
    "        'Book Title': book_title,\n",
    "        'Chunks': len(chunks),\n",
    "        'Avg Sentences': f\"{book_sentences / len(chunks):.1f}\",\n",
    "        'Avg Words': f\"{book_words / len(chunks):.0f}\",\n",
    "        'Avg Chars': f\"{book_chars / len(chunks):.0f}\"\n",
    "    })\n",
    "\n",
    "df_book_stats = pd.DataFrame(book_stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS BY BOOK\")\n",
    "print(\"=\"*80)\n",
    "print(df_book_stats.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE SEMANTIC CHUNKS\n",
      "================================================================================\n",
      "\n",
      "Chunk 1:\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 299f007b_p2_sc0\n",
      "Book: AI Engineering\n",
      "Chapter: Introduction\n",
      "Page: 2\n",
      "Size: 3 sentences, 62 words, 394 chars\n",
      "Has Context: Yes\n",
      "\n",
      "Text Preview (first 300 characters):\n",
      "“This book of fers a comprehensive, well-structured guide to the essential\n",
      "aspects of building generative AI systems. A must-read for any professional\n",
      "looking to scale AI across the enterprise.”\n",
      "Vittorio Cretella, former global CIO at P&G and Mars\n",
      "“Chip Huyen gets generative AI. She is a remarkable ...\n",
      "\n",
      "Context Preview (first 150 characters):\n",
      "Drawing on her deep expertise, AI Engineering is a comprehensive and\n",
      "holistic guide to building generative AI applications in production.”\n",
      "Luke Metz, ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 299f007b_p2_sc1\n",
      "Book: AI Engineering\n",
      "Chapter: Introduction\n",
      "Page: 2\n",
      "Size: 3 sentences, 94 words, 635 chars\n",
      "Has Context: Yes\n",
      "\n",
      "Text Preview (first 300 characters):\n",
      "Drawing on her deep expertise, AI Engineering is a comprehensive and\n",
      "holistic guide to building generative AI applications in production.”\n",
      "Luke Metz, cocreator of ChatGPT, former research manager at OpenAI\n",
      "AI Engineering\n",
      "Foundation models have enabled many new AI use cases while lowering the barrier...\n",
      "\n",
      "Context Preview (first 150 characters):\n",
      "She is a remarkable teacher and writer\n",
      "whose work has been instrumental in helping teams bring AI into production. AI application developers will disc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 299f007b_p2_sc2\n",
      "Book: AI Engineering\n",
      "Chapter: Introduction\n",
      "Page: 2\n",
      "Size: 3 sentences, 66 words, 462 chars\n",
      "Has Context: Yes\n",
      "\n",
      "Text Preview (first 300 characters):\n",
      "AI application developers will discover how to navigate\n",
      "the AI landscape, including models, datasets, evaluation benchmarks, and the seemingly infinite number\n",
      "of application patterns. The book also introduces a practical framework for developing an AI application\n",
      "and efficiently deploying it. • Unde...\n",
      "\n",
      "Context Preview (first 150 characters):\n",
      "In this accessible guide, author Chip Huyen discusses AI engineering: the process of building applications\n",
      "with readily available foundation models. P...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample chunks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE SEMANTIC CHUNKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, chunk in enumerate(all_chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"ID: {chunk['chunk_id']}\")\n",
    "    print(f\"Book: {chunk['book_title']}\")\n",
    "    print(f\"Chapter: {chunk['chapter']}\")\n",
    "    print(f\"Page: {chunk['page_number']}\")\n",
    "    print(f\"Size: {chunk['sentence_count']} sentences, {chunk['word_count']} words, {chunk['char_count']} chars\")\n",
    "    print(f\"Has Context: {'Yes' if chunk['context'] else 'No'}\")\n",
    "    print(f\"\\nText Preview (first 300 characters):\")\n",
    "    print(chunk['text'][:300] + \"...\")\n",
    "    if chunk['context']:\n",
    "        print(f\"\\nContext Preview (first 150 characters):\")\n",
    "        print(chunk['context'][:150] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Semantic Chunking:\n",
    "\n",
    "1. **Better coherence** - Chunks maintain topic boundaries\n",
    "2. **Improved retrieval** - More meaningful embeddings\n",
    "3. **Context enrichment** - Surrounding sentences provide additional context\n",
    "4. **Adaptive sizing** - Chunks vary based on content, not arbitrary limits\n",
    "\n",
    "### Output Files:\n",
    "\n",
    "- `all_chunks_semantic.json` - All semantic chunks\n",
    "- `[BookTitle]_chunks_semantic.json` - Per-book semantic chunks\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "**Notebook 3**: Generate embeddings using the `text_with_context` field and update ChromaDB\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

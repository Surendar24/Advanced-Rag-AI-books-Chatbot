{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Semantic Text Chunking\n",
    "\n",
    "## Purpose\n",
    "This notebook implements **semantic chunking** - an advanced RAG technique that creates chunks based on meaning and context rather than arbitrary character counts.\n",
    "\n",
    "## Semantic Chunking Benefits\n",
    "1. **Meaning-based boundaries** - Chunks break at natural semantic boundaries\n",
    "2. **Better retrieval** - More coherent chunks improve embedding quality\n",
    "3. **Context preservation** - Maintains topic coherence within chunks\n",
    "4. **Reduced fragmentation** - Avoids splitting related concepts\n",
    "\n",
    "## Process\n",
    "1. Load extracted text data from Notebook 1\n",
    "2. Use sentence embeddings to detect semantic boundaries\n",
    "3. Group sentences with similar embeddings into chunks\n",
    "4. Add context enrichment (surrounding context)\n",
    "5. Preserve metadata (book, chapter, page)\n",
    "6. Save semantic chunks for embedding generation\n",
    "\n",
    "## Output\n",
    "- Semantically coherent chunks with metadata\n",
    "- Context-enriched chunks for better retrieval\n",
    "- Ready for embedding generation in Notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(r\"d:\\AI Book RAG\")\n",
    "EXTRACTED_DIR = BASE_DIR / \"data\" / \"extracted\"\n",
    "CHUNKS_DIR = BASE_DIR / \"data\" / \"chunks\"\n",
    "\n",
    "# Create output directory\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Semantic chunking parameters\n",
    "SIMILARITY_THRESHOLD = 0.75  # Cosine similarity threshold for grouping sentences\n",
    "MIN_CHUNK_SENTENCES = 3      # Minimum sentences per chunk\n",
    "MAX_CHUNK_SENTENCES = 10     # Maximum sentences per chunk\n",
    "MIN_CHUNK_CHARS = 200        # Minimum characters per chunk\n",
    "MAX_CHUNK_CHARS = 1500       # Maximum characters per chunk\n",
    "CONTEXT_SENTENCES = 1        # Number of sentences to add as context before/after\n",
    "\n",
    "print(f\"Extracted Data Directory: {EXTRACTED_DIR}\")\n",
    "print(f\"Chunks Output Directory: {CHUNKS_DIR}\")\n",
    "print(f\"\\nSemantic Chunking Configuration:\")\n",
    "print(f\"  - Similarity Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  - Chunk Size: {MIN_CHUNK_SENTENCES}-{MAX_CHUNK_SENTENCES} sentences\")\n",
    "print(f\"  - Character Range: {MIN_CHUNK_CHARS}-{MAX_CHUNK_CHARS} chars\")\n",
    "print(f\"  - Context Enrichment: ±{CONTEXT_SENTENCES} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model for semantic similarity\n",
    "print(\"Loading sentence embedding model...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted data\n",
    "combined_file = EXTRACTED_DIR / \"all_books_combined.json\"\n",
    "print(f\"Loading extracted data from: {combined_file}\")\n",
    "\n",
    "with open(combined_file, 'r', encoding='utf-8') as f:\n",
    "    all_books_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded data for {len(all_books_data)} books\")\n",
    "total_pages = sum(book['total_pages'] for book in all_books_data)\n",
    "print(f\"✓ Total pages to process: {total_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Clean text\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    # Replace multiple newlines with double newline\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    # Remove leading/trailing whitespace from each line\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "    # Remove standalone page numbers\n",
    "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    # Clean up again\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Helper: Generate chunk ID\n",
    "def generate_chunk_id(book_title: str, page_num: int, chunk_index: int) -> str:\n",
    "    \"\"\"Generate unique chunk identifier\"\"\"\n",
    "    book_hash = hashlib.md5(book_title.encode()).hexdigest()[:8]\n",
    "    return f\"{book_hash}_p{page_num}_sc{chunk_index}\"\n",
    "\n",
    "# Helper: Calculate semantic similarity between sentences\n",
    "def calculate_sentence_similarities(sentences: List[str], embeddings: np.ndarray) -> List[float]:\n",
    "    \"\"\"Calculate cosine similarity between consecutive sentences\"\"\"\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        sim = cosine_similarity(\n",
    "            embeddings[i].reshape(1, -1),\n",
    "            embeddings[i + 1].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append(sim)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core: Semantic chunking function\n",
    "def semantic_chunk_text(\n",
    "    text: str,\n",
    "    embedding_model: SentenceTransformer,\n",
    "    similarity_threshold: float = SIMILARITY_THRESHOLD,\n",
    "    min_sentences: int = MIN_CHUNK_SENTENCES,\n",
    "    max_sentences: int = MAX_CHUNK_SENTENCES,\n",
    "    min_chars: int = MIN_CHUNK_CHARS,\n",
    "    max_chars: int = MAX_CHUNK_CHARS\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into semantic chunks based on sentence similarity.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Split text into sentences\n",
    "    2. Generate embeddings for each sentence\n",
    "    3. Calculate similarity between consecutive sentences\n",
    "    4. Group sentences where similarity > threshold\n",
    "    5. Respect min/max constraints\n",
    "    \"\"\"\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    if len(sentences) < min_sentences:\n",
    "        return [text] if len(text) >= min_chars else []\n",
    "    \n",
    "    # Generate embeddings for all sentences\n",
    "    embeddings = embedding_model.encode(sentences, show_progress_bar=False)\n",
    "    \n",
    "    # Calculate similarities between consecutive sentences\n",
    "    similarities = calculate_sentence_similarities(sentences, embeddings)\n",
    "    \n",
    "    # Find semantic boundaries (where similarity drops below threshold)\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_chars = len(sentences[0])\n",
    "    \n",
    "    for i, (sentence, similarity) in enumerate(zip(sentences[1:], similarities), 1):\n",
    "        sentence_len = len(sentence)\n",
    "        \n",
    "        # Check if we should start a new chunk\n",
    "        should_break = (\n",
    "            # Semantic boundary detected\n",
    "            similarity < similarity_threshold or\n",
    "            # Max sentences reached\n",
    "            len(current_chunk) >= max_sentences or\n",
    "            # Max chars would be exceeded\n",
    "            (current_chars + sentence_len > max_chars and len(current_chunk) >= min_sentences)\n",
    "        )\n",
    "        \n",
    "        if should_break and len(current_chunk) >= min_sentences and current_chars >= min_chars:\n",
    "            # Save current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chars = sentence_len\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_chars += sentence_len\n",
    "    \n",
    "    # Add final chunk if it meets criteria\n",
    "    if current_chunk and len(current_chunk) >= min_sentences and current_chars >= min_chars:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    elif current_chunk and chunks:  # Merge small final chunk with previous\n",
    "        chunks[-1] += ' ' + ' '.join(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context enrichment: Add surrounding context to chunks\n",
    "def enrich_chunk_with_context(\n",
    "    chunk_text: str,\n",
    "    page_text: str,\n",
    "    context_sentences: int = CONTEXT_SENTENCES\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Add context from surrounding sentences to improve retrieval.\n",
    "    Returns dict with main chunk and context.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(page_text)\n",
    "    chunk_sentences = sent_tokenize(chunk_text)\n",
    "    \n",
    "    if not chunk_sentences:\n",
    "        return {'main': chunk_text, 'context': ''}\n",
    "    \n",
    "    # Find where chunk starts in page\n",
    "    first_chunk_sent = chunk_sentences[0]\n",
    "    try:\n",
    "        start_idx = sentences.index(first_chunk_sent)\n",
    "    except ValueError:\n",
    "        return {'main': chunk_text, 'context': ''}\n",
    "    \n",
    "    # Get context before\n",
    "    context_before = []\n",
    "    for i in range(max(0, start_idx - context_sentences), start_idx):\n",
    "        context_before.append(sentences[i])\n",
    "    \n",
    "    # Get context after\n",
    "    end_idx = start_idx + len(chunk_sentences)\n",
    "    context_after = []\n",
    "    for i in range(end_idx, min(len(sentences), end_idx + context_sentences)):\n",
    "        context_after.append(sentences[i])\n",
    "    \n",
    "    context = ' '.join(context_before + context_after)\n",
    "    \n",
    "    return {\n",
    "        'main': chunk_text,\n",
    "        'context': context\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main: Process all books with semantic chunking\n",
    "def create_semantic_chunks_from_books(books_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process all books and create semantic chunks with context enrichment.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    chunk_global_index = 0\n",
    "    \n",
    "    for book_data in books_data:\n",
    "        book_title = book_data['book_title']\n",
    "        pages = book_data['pages']\n",
    "        \n",
    "        print(f\"\\nProcessing: {book_title}\")\n",
    "        print(f\"Pages: {len(pages)}\")\n",
    "        \n",
    "        book_chunks = []\n",
    "        \n",
    "        for page_data in tqdm(pages, desc=f\"Semantic chunking {book_title}\"):\n",
    "            # Clean text\n",
    "            cleaned_text = clean_text(page_data['text'])\n",
    "            \n",
    "            if len(cleaned_text) < MIN_CHUNK_CHARS:\n",
    "                continue\n",
    "            \n",
    "            # Create semantic chunks\n",
    "            text_chunks = semantic_chunk_text(\n",
    "                cleaned_text,\n",
    "                embedding_model\n",
    "            )\n",
    "            \n",
    "            # Process each chunk\n",
    "            for chunk_index, chunk_text in enumerate(text_chunks):\n",
    "                # Add context enrichment\n",
    "                enriched = enrich_chunk_with_context(chunk_text, cleaned_text)\n",
    "                \n",
    "                # Combine main chunk with context for embedding\n",
    "                full_text = chunk_text\n",
    "                if enriched['context']:\n",
    "                    full_text = f\"{enriched['context']} {chunk_text}\"\n",
    "                \n",
    "                # Generate chunk ID\n",
    "                chunk_id = generate_chunk_id(\n",
    "                    book_title,\n",
    "                    page_data['page_number'],\n",
    "                    chunk_index\n",
    "                )\n",
    "                \n",
    "                # Create chunk object\n",
    "                chunk = {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"global_index\": chunk_global_index,\n",
    "                    \"book_title\": book_title,\n",
    "                    \"chapter\": page_data['chapter'],\n",
    "                    \"page_number\": page_data['page_number'],\n",
    "                    \"chunk_index\": chunk_index,\n",
    "                    \"text\": chunk_text,  # Main chunk for display\n",
    "                    \"text_with_context\": full_text,  # For embedding\n",
    "                    \"context\": enriched['context'],\n",
    "                    \"char_count\": len(chunk_text),\n",
    "                    \"word_count\": len(chunk_text.split()),\n",
    "                    \"sentence_count\": len(sent_tokenize(chunk_text)),\n",
    "                    \"citation\": f\"[{book_title}, {page_data['chapter']}, Page {page_data['page_number']}]\",\n",
    "                    \"chunking_method\": \"semantic\"\n",
    "                }\n",
    "                \n",
    "                book_chunks.append(chunk)\n",
    "                all_chunks.append(chunk)\n",
    "                chunk_global_index += 1\n",
    "        \n",
    "        print(f\"✓ Created {len(book_chunks)} semantic chunks from {book_title}\")\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute semantic chunking\n",
    "print(\"Starting semantic chunking process...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_chunks = create_semantic_chunks_from_books(all_books_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Semantic chunking complete!\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save semantic chunks\n",
    "print(\"\\nSaving semantic chunks...\")\n",
    "\n",
    "# Save all chunks\n",
    "chunks_file = CHUNKS_DIR / \"all_chunks_semantic.json\"\n",
    "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved: {chunks_file.name}\")\n",
    "\n",
    "# Save by book\n",
    "chunks_by_book = {}\n",
    "for chunk in all_chunks:\n",
    "    book_title = chunk['book_title']\n",
    "    if book_title not in chunks_by_book:\n",
    "        chunks_by_book[book_title] = []\n",
    "    chunks_by_book[book_title].append(chunk)\n",
    "\n",
    "for book_title, chunks in chunks_by_book.items():\n",
    "    book_title_safe = re.sub(r'[^a-zA-Z0-9\\s]', '', book_title)\n",
    "    book_title_safe = '_'.join(book_title_safe.split())\n",
    "    \n",
    "    book_chunks_file = CHUNKS_DIR / f\"{book_title_safe}_chunks_semantic.json\"\n",
    "    with open(book_chunks_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Saved: {book_chunks_file.name} ({len(chunks)} chunks)\")\n",
    "\n",
    "print(\"\\nAll semantic chunks saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC CHUNKING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_chunks = len(all_chunks)\n",
    "total_chars = sum(chunk['char_count'] for chunk in all_chunks)\n",
    "total_words = sum(chunk['word_count'] for chunk in all_chunks)\n",
    "total_sentences = sum(chunk['sentence_count'] for chunk in all_chunks)\n",
    "\n",
    "avg_chars = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "avg_words = total_words / total_chunks if total_chunks > 0 else 0\n",
    "avg_sentences = total_sentences / total_chunks if total_chunks > 0 else 0\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"  Total Chunks: {total_chunks:,}\")\n",
    "print(f\"  Total Characters: {total_chars:,}\")\n",
    "print(f\"  Total Words: {total_words:,}\")\n",
    "print(f\"  Total Sentences: {total_sentences:,}\")\n",
    "print(f\"\\nAverage per Chunk:\")\n",
    "print(f\"  Characters: {avg_chars:.1f}\")\n",
    "print(f\"  Words: {avg_words:.1f}\")\n",
    "print(f\"  Sentences: {avg_sentences:.1f}\")\n",
    "\n",
    "# Chunks with context enrichment\n",
    "chunks_with_context = sum(1 for chunk in all_chunks if chunk['context'])\n",
    "print(f\"\\nContext Enrichment:\")\n",
    "print(f\"  Chunks with context: {chunks_with_context} ({chunks_with_context/total_chunks*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by book\n",
    "book_stats = []\n",
    "for book_title, chunks in chunks_by_book.items():\n",
    "    book_chars = sum(chunk['char_count'] for chunk in chunks)\n",
    "    book_words = sum(chunk['word_count'] for chunk in chunks)\n",
    "    book_sentences = sum(chunk['sentence_count'] for chunk in chunks)\n",
    "    \n",
    "    book_stats.append({\n",
    "        'Book Title': book_title,\n",
    "        'Chunks': len(chunks),\n",
    "        'Avg Sentences': f\"{book_sentences / len(chunks):.1f}\",\n",
    "        'Avg Words': f\"{book_words / len(chunks):.0f}\",\n",
    "        'Avg Chars': f\"{book_chars / len(chunks):.0f}\"\n",
    "    })\n",
    "\n",
    "df_book_stats = pd.DataFrame(book_stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS BY BOOK\")\n",
    "print(\"=\"*80)\n",
    "print(df_book_stats.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample chunks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE SEMANTIC CHUNKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, chunk in enumerate(all_chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"ID: {chunk['chunk_id']}\")\n",
    "    print(f\"Book: {chunk['book_title']}\")\n",
    "    print(f\"Chapter: {chunk['chapter']}\")\n",
    "    print(f\"Page: {chunk['page_number']}\")\n",
    "    print(f\"Size: {chunk['sentence_count']} sentences, {chunk['word_count']} words, {chunk['char_count']} chars\")\n",
    "    print(f\"Has Context: {'Yes' if chunk['context'] else 'No'}\")\n",
    "    print(f\"\\nText Preview (first 300 characters):\")\n",
    "    print(chunk['text'][:300] + \"...\")\n",
    "    if chunk['context']:\n",
    "        print(f\"\\nContext Preview (first 150 characters):\")\n",
    "        print(chunk['context'][:150] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "✅ Semantic chunking complete!\n",
    "\n",
    "### Advantages of Semantic Chunking:\n",
    "1. **Better coherence** - Chunks maintain topic boundaries\n",
    "2. **Improved retrieval** - More meaningful embeddings\n",
    "3. **Context enrichment** - Surrounding sentences provide additional context\n",
    "4. **Adaptive sizing** - Chunks vary based on content, not arbitrary limits\n",
    "\n",
    "### Output Files:\n",
    "- `all_chunks_semantic.json` - All semantic chunks\n",
    "- `[BookTitle]_chunks_semantic.json` - Per-book semantic chunks\n",
    "\n",
    "### What's Next:\n",
    "**Notebook 3**: Generate embeddings using the `text_with_context` field and update ChromaDB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Embedding Generation and Vector Database Setup\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook generates embeddings for text chunks and stores them in ChromaDB for efficient semantic search.\n",
    "\n",
    "## Process\n",
    "\n",
    "1. Load chunked text data from Notebook 2\n",
    "2. Initialize embedding model (sentence-transformers)\n",
    "3. Generate embeddings for all text chunks\n",
    "4. Set up ChromaDB vector database\n",
    "5. Store chunks with embeddings and metadata\n",
    "6. Test similarity search functionality\n",
    "\n",
    "## Output\n",
    "\n",
    "- ChromaDB database with embedded chunks\n",
    "- Embedding statistics and performance metrics\n",
    "- Sample similarity search results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Embedding and vector database\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks Directory: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\data\\semantic_chunks\n",
      "ChromaDB Directory: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\chroma_db\n",
      "\n",
      "Configuration:\n",
      "  Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Batch Size: 32\n",
      "  Collection Name: ai_books_collection\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration: Set up paths and parameters\n",
    "BASE_DIR = Path(r\"c:\\\\Users\\\\jagth\\\\Downloads\\\\New folder\\\\ai-books-rag-chatbot\\\\Books_pdf\")\n",
    "CHUNKS_DIR = BASE_DIR / \"data\" / \"semantic_chunks\"\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "\n",
    "# Create ChromaDB directory\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Embedding model configuration\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "BATCH_SIZE = 32  # Process embeddings in batches for efficiency\n",
    "\n",
    "# ChromaDB collection name\n",
    "COLLECTION_NAME = \"ai_books_collection\"\n",
    "\n",
    "print(f\"Chunks Directory: {CHUNKS_DIR}\")\n",
    "print(f\"ChromaDB Directory: {CHROMA_DIR}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Collection Name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\data\\semantic_chunks\\all_chunks_semantic.json\n",
      "✓ Loaded 10620 chunks\n",
      "\n",
      "Sample chunk structure:\n",
      "  chunk_id: 299f007b_p2_sc0\n",
      "  global_index: 0\n",
      "  book_title: AI Engineering\n",
      "  chapter: Introduction\n",
      "  page_number: 2\n",
      "  chunk_index: 0\n",
      "  text: [text content - 394 chars]\n",
      "  text_with_context: Drawing on her deep expertise, AI Engineering is a comprehensive and\n",
      "holistic guide to building generative AI applications in production.”\n",
      "Luke Metz, cocreator of ChatGPT, former research manager at OpenAI\n",
      "AI Engineering\n",
      "Foundation models have enabled many new AI use cases while lowering the barriers to entry for\n",
      "building AI products. “This book of fers a comprehensive, well-structured guide to the essential\n",
      "aspects of building generative AI systems. A must-read for any professional\n",
      "looking to scale AI across the enterprise.”\n",
      "Vittorio Cretella, former global CIO at P&G and Mars\n",
      "“Chip Huyen gets generative AI. She is a remarkable teacher and writer\n",
      "whose work has been instrumental in helping teams bring AI into production.\n",
      "  context: Drawing on her deep expertise, AI Engineering is a comprehensive and\n",
      "holistic guide to building generative AI applications in production.”\n",
      "Luke Metz, cocreator of ChatGPT, former research manager at OpenAI\n",
      "AI Engineering\n",
      "Foundation models have enabled many new AI use cases while lowering the barriers to entry for\n",
      "building AI products.\n",
      "  char_count: 394\n",
      "  word_count: 62\n",
      "  sentence_count: 3\n",
      "  citation: [AI Engineering, Introduction, Page 2]\n",
      "  chunking_method: semantic\n"
     ]
    }
   ],
   "source": [
    "# Load chunked data from Notebook 2\n",
    "chunks_file = CHUNKS_DIR / \"all_chunks_semantic.json\"\n",
    "\n",
    "print(f\"Loading chunks from: {chunks_file}\")\n",
    "\n",
    "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "    all_chunks = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(all_chunks)} chunks\")\n",
    "\n",
    "# Display sample chunk structure\n",
    "if all_chunks:\n",
    "    print(\"\\nSample chunk structure:\")\n",
    "    sample_chunk = all_chunks[0]\n",
    "    for key in sample_chunk.keys():\n",
    "        if key != 'text':  # Don't print full text\n",
    "            print(f\"  {key}: {sample_chunk[key]}\")\n",
    "        else:\n",
    "            print(f\"  {key}: [text content - {len(sample_chunk[key])} chars]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "This may take a moment on first run (downloading model)...\n",
      "\n",
      "✓ Model loaded successfully in 3.07 seconds\n",
      "  Model dimension: 384\n",
      "  Max sequence length: 256\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(f\"\\nInitializing embedding model: {EMBEDDING_MODEL}\")\n",
    "print(\"This may take a moment on first run (downloading model)...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the sentence transformer model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Model loaded successfully in {load_time:.2f} seconds\")\n",
    "print(f\"  Model dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max sequence length: {embedding_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test embedding generation:\n",
      "  Input text: 'What is machine learning and how does it work?'\n",
      "  Embedding shape: (384,)\n",
      "  Embedding type: <class 'numpy.ndarray'>\n",
      "  First 5 values: [-0.0373502   0.0123879   0.01443943  0.01891052  0.03915355]\n"
     ]
    }
   ],
   "source": [
    "# Test embedding generation with a sample\n",
    "sample_text = \"What is machine learning and how does it work?\"\n",
    "sample_embedding = embedding_model.encode(sample_text)\n",
    "\n",
    "print(f\"\\nTest embedding generation:\")\n",
    "print(f\"  Input text: '{sample_text}'\")\n",
    "print(f\"  Embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"  Embedding type: {type(sample_embedding)}\")\n",
    "print(f\"  First 5 values: {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing ChromaDB client...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDB client initialized\n",
      "  Persist directory: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\chroma_db\n",
      "  Existing collections: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client\n",
    "print(f\"\\nInitializing ChromaDB client...\")\n",
    "\n",
    "# Create persistent client\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(CHROMA_DIR),\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"✓ ChromaDB client initialized\")\n",
    "print(f\"  Persist directory: {CHROMA_DIR}\")\n",
    "\n",
    "# List existing collections (if any)\n",
    "existing_collections = chroma_client.list_collections()\n",
    "print(f\"  Existing collections: {len(existing_collections)}\")\n",
    "for col in existing_collections:\n",
    "    print(f\"    - {col.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up collection: ai_books_collection\n",
      "  No existing collection to delete\n",
      "✓ Collection created: ai_books_collection\n",
      "  Metadata: {'description': 'AI Books RAG Collection', 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2', 'created_at': '2026-01-10 17:30:00'}\n"
     ]
    }
   ],
   "source": [
    "# Create or get collection\n",
    "print(f\"\\nSetting up collection: {COLLECTION_NAME}\")\n",
    "\n",
    "# Delete existing collection if it exists (for clean start)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=COLLECTION_NAME)\n",
    "    print(f\"  Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    print(f\"  No existing collection to delete\")\n",
    "\n",
    "# Create new collection with custom embedding function\n",
    "collection = chroma_client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\n",
    "        \"description\": \"AI Books RAG Collection\",\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ Collection created: {COLLECTION_NAME}\")\n",
    "print(f\"  Metadata: {collection.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Generate embeddings in batches\n",
    "def generate_embeddings_batch(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = embedding_model.encode(\n",
    "            batch,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for 10620 chunks...\n",
      "This may take several minutes depending on your hardware.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 332/332 [06:51<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Embeddings generated successfully\n",
      "  Total time: 412.12 seconds\n",
      "  Time per chunk: 0.0388 seconds\n",
      "  Embeddings shape: (10620, 384)\n",
      "  Memory usage: 15.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(all_chunks)} chunks...\")\n",
    "print(f\"This may take several minutes depending on your hardware.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract texts from chunks\n",
    "chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = generate_embeddings_batch(chunk_texts, batch_size=BATCH_SIZE)\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Embeddings generated successfully\")\n",
    "print(f\"  Total time: {embedding_time:.2f} seconds\")\n",
    "print(f\"  Time per chunk: {embedding_time / len(all_chunks):.4f} seconds\")\n",
    "print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  Memory usage: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for ChromaDB...\n",
      "✓ Data prepared\n",
      "  IDs: 10620\n",
      "  Documents: 10620\n",
      "  Metadatas: 10620\n",
      "  Embeddings: 10620\n",
      "\n",
      "Sample metadata fields:\n",
      "  - book_title\n",
      "  - chapter\n",
      "  - page_number\n",
      "  - chunk_index\n",
      "  - citation\n",
      "  - char_count\n",
      "  - word_count\n",
      "  - sentence_count\n",
      "  - chunking_method\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for ChromaDB insertion\n",
    "print(f\"\\nPreparing data for ChromaDB...\")\n",
    "\n",
    "# Extract IDs, documents, metadatas, and embeddings\n",
    "ids = [chunk['chunk_id'] for chunk in all_chunks]\n",
    "documents = [chunk['text'] for chunk in all_chunks]\n",
    "\n",
    "# Handle both old (character-based) and new (semantic) chunk formats\n",
    "metadatas = []\n",
    "for chunk in all_chunks:\n",
    "    metadata = {\n",
    "        \"book_title\": chunk['book_title'],\n",
    "        \"chapter\": chunk['chapter'],\n",
    "        \"page_number\": chunk['page_number'],\n",
    "        \"chunk_index\": chunk['chunk_index'],\n",
    "        \"citation\": chunk['citation'],\n",
    "        \"char_count\": chunk['char_count'],\n",
    "        \"word_count\": chunk['word_count']\n",
    "    }\n",
    "    \n",
    "    # Add optional fields if they exist\n",
    "    if 'token_count' in chunk:\n",
    "        metadata['token_count'] = chunk['token_count']\n",
    "    if 'sentence_count' in chunk:\n",
    "        metadata['sentence_count'] = chunk['sentence_count']\n",
    "    if 'chunking_method' in chunk:\n",
    "        metadata['chunking_method'] = chunk['chunking_method']\n",
    "    \n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Convert embeddings to list format for ChromaDB\n",
    "embeddings_list = embeddings.tolist()\n",
    "\n",
    "print(f\"✓ Data prepared\")\n",
    "print(f\"  IDs: {len(ids)}\")\n",
    "print(f\"  Documents: {len(documents)}\")\n",
    "print(f\"  Metadatas: {len(metadatas)}\")\n",
    "print(f\"  Embeddings: {len(embeddings_list)}\")\n",
    "\n",
    "# Show sample metadata\n",
    "if metadatas:\n",
    "    print(f\"\\nSample metadata fields:\")\n",
    "    for key in metadatas[0].keys():\n",
    "        print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding data to ChromaDB collection...\n",
      "Processing in batches of 32...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting into ChromaDB:   0%|          | 0/332 [00:00<?, ?it/s]Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "Inserting into ChromaDB: 100%|██████████| 332/332 [00:15<00:00, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data inserted successfully\n",
      "  Total time: 15.64 seconds\n",
      "  Collection count: 10620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add data to ChromaDB collection in batches\n",
    "print(f\"\\nAdding data to ChromaDB collection...\")\n",
    "print(f\"Processing in batches of {BATCH_SIZE}...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ChromaDB has a limit on batch size, so we insert in chunks\n",
    "for i in tqdm(range(0, len(ids), BATCH_SIZE), desc=\"Inserting into ChromaDB\"):\n",
    "    batch_end = min(i + BATCH_SIZE, len(ids))\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids[i:batch_end],\n",
    "        documents=documents[i:batch_end],\n",
    "        metadatas=metadatas[i:batch_end],\n",
    "        embeddings=embeddings_list[i:batch_end]\n",
    "    )\n",
    "\n",
    "insert_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Data inserted successfully\")\n",
    "print(f\"  Total time: {insert_time:.2f} seconds\")\n",
    "print(f\"  Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying ChromaDB collection...\n",
      "✓ Collection verification:\n",
      "  Name: ai_books_collection\n",
      "  Total documents: 10620\n",
      "  Metadata: {'description': 'AI Books RAG Collection', 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2', 'created_at': '2026-01-10 17:30:00'}\n",
      "\n",
      "✓ Data integrity verified: 10620 chunks stored successfully\n"
     ]
    }
   ],
   "source": [
    "# Verify collection\n",
    "print(f\"\\nVerifying ChromaDB collection...\")\n",
    "\n",
    "collection_count = collection.count()\n",
    "print(f\"✓ Collection verification:\")\n",
    "print(f\"  Name: {collection.name}\")\n",
    "print(f\"  Total documents: {collection_count}\")\n",
    "print(f\"  Metadata: {collection.metadata}\")\n",
    "\n",
    "# Verify data integrity\n",
    "assert collection_count == len(all_chunks), \"Mismatch between chunks and collection count!\"\n",
    "print(f\"\\n✓ Data integrity verified: {collection_count} chunks stored successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING SIMILARITY SEARCH\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Query: 'What is transfer learning?'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "  Book: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "  Chapter: Chapter 13: to increase the accuracy of an NLP model that utilizes\n",
      "  Page: 403\n",
      "  Citation: [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403]\n",
      "  Distance: 0.6185\n",
      "  Text preview: Training a\n",
      "CNN to solve a more perceptual problem requires more training\n",
      "images and commensurately more compute power. Transfer\n",
      "learning is a practical alternative to training CNNs from\n",
      "scratch. It us...\n",
      "\n",
      "Result 2:\n",
      "  Book: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "  Chapter: Chapter 10: . Image Classification\n",
      "  Page: 366\n",
      "  Citation: [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 10: . Image Classification, Page 366]\n",
      "  Distance: 0.6956\n",
      "  Text preview: But all is not lost. A technique called transfer learning\n",
      "enables pretrained CNNs to be repurposed to solve domain-\n",
      "specific problems. The repurposing can be done on an ordinary\n",
      "CPU; no GPU required. ...\n",
      "\n",
      "Result 3:\n",
      "  Book: AI Engineering\n",
      "  Chapter: Chapter 7: Finetuning\n",
      "  Page: 332\n",
      "  Citation: [AI Engineering, Chapter 7: Finetuning, Page 332]\n",
      "  Distance: 0.6959\n",
      "  Text preview: The goal of finetuning is to get this model to perform well enough for your\n",
      "specific task. Finetuning is one way to do transfer learning, a concept first introduced by Bozinov‐\n",
      "ski and Fulgosi in 1976...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Explain the transformer architecture'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Book: AI Engineering\n",
      "  Chapter: Chapter 1: , a model’s training process is often divided into pre-\n",
      "  Page: 82\n",
      "  Citation: [AI Engineering, Chapter 1: , a model’s training process is often divided into pre-, Page 82]\n",
      "  Distance: 0.4803\n",
      "  Text preview: It addresses many limitations of the previous architectures,\n",
      "which contributed to its popularity. However, the transformer architecture has its\n",
      "own limitations. This section analyzes the transformer a...\n",
      "\n",
      "Result 2:\n",
      "  Book: NLP with Transformer models\n",
      "  Chapter: Chapter 3: Transformer Anatomy\n",
      "  Page: 81\n",
      "  Citation: [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81]\n",
      "  Distance: 0.5656\n",
      "  Text preview: CHAPTER 3\n",
      "Transformer Anatomy\n",
      "In Chapter 2, we saw what it takes to fine-tune and evaluate a transformer. Now let’s\n",
      "take a look at how they work under the hood. In this chapter we’ll explore the main\n",
      "...\n",
      "\n",
      "Result 3:\n",
      "  Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "  Chapter: Chapter 6: .\n",
      "  Page: 117\n",
      "  Citation: [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 117]\n",
      "  Distance: 0.6182\n",
      "  Text preview: This spans training on larger datasets and opti‐\n",
      "mizations for the training process and learning rates to use, but it also extends to the\n",
      "architecture itself. At the time of writing, a lot of the idea...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'How do I train a neural network?'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Book: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "  Chapter: Chapter 8: . Deep Learning\n",
      "  Page: 291\n",
      "  Citation: [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 8: . Deep Learning, Page 291]\n",
      "  Distance: 0.7513\n",
      "  Text preview: Now that you understand how neural networks work, the next\n",
      "step is to learn how to build and train them. For that, data\n",
      "scientists rely on frameworks such as Keras and TensorFlow. Chapter 9 begins a d...\n",
      "\n",
      "Result 2:\n",
      "  Book: Hands-On Machine Learning with Scikit-Learn and PyTorch\n",
      "  Chapter: Chapter 10: . Building Neural\n",
      "  Page: 489\n",
      "  Citation: [Hands-On Machine Learning with Scikit-Learn and PyTorch, Chapter 10: . Building Neural, Page 489]\n",
      "  Distance: 0.7559\n",
      "  Text preview: In this chapter, you will learn how to train, evaluate, fine-tune, optimize, and save\n",
      "neural nets with PyTorch. We will start by getting familiar with the core building blocks\n",
      "of PyTorch, namely tenso...\n",
      "\n",
      "Result 3:\n",
      "  Book: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "  Chapter: Chapter 8: . Deep Learning\n",
      "  Page: 290\n",
      "  Citation: [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 8: . Deep Learning, Page 290]\n",
      "  Distance: 0.7752\n",
      "  Text preview: to the challenges inherent to training neural networks, see the\n",
      "article “How Neural Networks Are Trained”. Neural networks are fundamentally simple. Training them is\n",
      "mathematically complex. Fortunatel...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'What are the best practices for fine-tuning LLMs?'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Book: LLM Engineers Handbook\n",
      "  Chapter: Chapter 1: 21\n",
      "  Page: 50\n",
      "  Citation: [LLM Engineers Handbook, Chapter 1: 21, Page 50]\n",
      "  Distance: 0.6445\n",
      "  Text preview: • How do you scale the fine-tuning algorithm on LLMs and datasets of various sizes? • How do you pick an LLM production candidate from multiple experiments? • How do you test the LLM to decide whether...\n",
      "\n",
      "Result 2:\n",
      "  Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "  Chapter: Introduction\n",
      "  Page: 11\n",
      "  Citation: [Hands-On Large Language Models Language Understanding and Generation, Introduction, Page 11]\n",
      "  Distance: 0.6669\n",
      "  Text preview: Fine-Tuning Generation Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n",
      "The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and\n",
      "Preferen...\n",
      "\n",
      "Result 3:\n",
      "  Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "  Chapter: Chapter 12: Fine-Tuning Generation Models\n",
      "  Page: 379\n",
      "  Citation: [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 379]\n",
      "  Distance: 0.6801\n",
      "  Text preview: Figure 12-3. The three steps of creating a high-quality LLM. In this chapter, we use a base model that was already trained on massive datasets and\n",
      "explore how we can fine-tune it using both fine-tunin...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search functionality\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TESTING SIMILARITY SEARCH\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is transfer learning?\",\n",
    "    \"Explain the transformer architecture\",\n",
    "    \"How do I train a neural network?\",\n",
    "    \"What are the best practices for fine-tuning LLMs?\"\n",
    "]\n",
    "\n",
    "# Number of results to retrieve\n",
    "top_k = 3\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Book: {metadata['book_title']}\")\n",
    "        print(f\"  Chapter: {metadata['chapter']}\")\n",
    "        print(f\"  Page: {metadata['page_number']}\")\n",
    "        print(f\"  Citation: {metadata['citation']}\")\n",
    "        print(f\"  Distance: {distance:.4f}\")\n",
    "        print(f\"  Text preview: {doc[:200]}...\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING FILTERED SEARCH (by book)\n",
      "================================================================================\n",
      "\n",
      "Query: 'What is machine learning?'\n",
      "Filter: Book = 'Hands-On Large Language Models Language Understanding and Generation'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "  Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "  Chapter: Chapter 1: An Introduction to Large Language Models\n",
      "  Page: 26\n",
      "  Distance: 0.9153\n",
      "  Text preview: Here is a more formal definition by one of the founders of the artificial intelligence\n",
      "discipline:\n",
      "[Artificial intelligence is] the science and engineering of making intelligent machines,\n",
      "especially i...\n",
      "\n",
      "Result 2:\n",
      "  Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "  Chapter: Introduction\n",
      "  Page: 14\n",
      "  Distance: 1.0178\n",
      "  Text preview: We start with an overview of the field and common techniques (see Chap‐\n",
      "ter 1) before moving over to two central components of these models, tokenization\n",
      "and embeddings (see Chapter 2). We finish this...\n",
      "\n",
      "Result 3:\n",
      "  Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "  Chapter: Chapter 4: ), clustering tasks (see Chapter 5), and semantic search (see Chapter 8).\n",
      "  Page: 47\n",
      "  Distance: 1.0700\n",
      "  Text preview: We will explore how other\n",
      "models, such as embedding models, representation models, and even bag-of-words\n",
      "can be used to empower LLMs. The Training Paradigm of Large Language Models\n",
      "Traditional machine...\n"
     ]
    }
   ],
   "source": [
    "# Test filtering by book\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TESTING FILTERED SEARCH (by book)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get unique book titles\n",
    "book_titles = list(set(chunk['book_title'] for chunk in all_chunks))\n",
    "test_book = book_titles[0] if book_titles else None\n",
    "\n",
    "if test_book:\n",
    "    query = \"What is machine learning?\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Filter: Book = '{test_book}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search with filter\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3,\n",
    "        where={\"book_title\": test_book},\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Book: {metadata['book_title']}\")\n",
    "        print(f\"  Chapter: {metadata['chapter']}\")\n",
    "        print(f\"  Page: {metadata['page_number']}\")\n",
    "        print(f\"  Distance: {distance:.4f}\")\n",
    "        print(f\"  Text preview: {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ChromaDB Collection:\n",
      "  Name: ai_books_collection\n",
      "  Total documents: 10,620\n",
      "  Embedding dimension: 384\n",
      "  Database size: 91.86 MB\n",
      "\n",
      "Books in Collection:\n",
      "  AI Engineering: 2,938 chunks\n",
      "  LLM Engineers Handbook: 1,862 chunks\n",
      "  Hands-On Machine Learning with Scikit-Learn and PyTorch: 1,726 chunks\n",
      "  Applied-Machine-Learning-and-AI-for-Engineers: 1,422 chunks\n",
      "  Hands-On Large Language Models Language Understanding and Generation: 1,394 chunks\n",
      "  NLP with Transformer models: 1,278 chunks\n",
      "\n",
      "Performance Metrics:\n",
      "  Embedding generation: 412.12 seconds\n",
      "  Database insertion: 15.64 seconds\n",
      "  Total processing time: 427.76 seconds\n",
      "  Average time per chunk: 0.0403 seconds\n",
      "\n",
      "================================================================================\n",
      "✓ Vector database setup complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display final statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Collection statistics\n",
    "print(f\"ChromaDB Collection:\")\n",
    "print(f\"  Name: {COLLECTION_NAME}\")\n",
    "print(f\"  Total documents: {collection.count():,}\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"  Database size: {sum(f.stat().st_size for f in CHROMA_DIR.rglob('*') if f.is_file()) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Books statistics\n",
    "books_count = {}\n",
    "for chunk in all_chunks:\n",
    "    book = chunk['book_title']\n",
    "    books_count[book] = books_count.get(book, 0) + 1\n",
    "\n",
    "print(f\"\\nBooks in Collection:\")\n",
    "for book, count in sorted(books_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {book}: {count:,} chunks\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Embedding generation: {embedding_time:.2f} seconds\")\n",
    "print(f\"  Database insertion: {insert_time:.2f} seconds\")\n",
    "print(f\"  Total processing time: {embedding_time + insert_time:.2f} seconds\")\n",
    "print(f\"  Average time per chunk: {(embedding_time + insert_time) / len(all_chunks):.4f} seconds\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Vector database setup complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Configuration saved to: c:\\Users\\jagth\\Downloads\\New folder\\ai-books-rag-chatbot\\Books_pdf\\vectordb_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save configuration for later use\n",
    "config = {\n",
    "    \"collection_name\": COLLECTION_NAME,\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"embedding_dimension\": int(embeddings.shape[1]),\n",
    "    \"total_chunks\": len(all_chunks),\n",
    "    \"chroma_path\": str(CHROMA_DIR),\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"books\": list(books_count.keys())\n",
    "}\n",
    "\n",
    "config_file = BASE_DIR / \"vectordb_config.json\"\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Configuration saved to: {config_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

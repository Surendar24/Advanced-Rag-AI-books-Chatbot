{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Embedding Generation and Vector Database Setup\n",
    "\n",
    "## Purpose\n",
    "This notebook generates embeddings for text chunks and stores them in ChromaDB for efficient semantic search.\n",
    "\n",
    "## Process\n",
    "1. Load chunked text data from Notebook 2\n",
    "2. Initialize embedding model (sentence-transformers)\n",
    "3. Generate embeddings for all text chunks\n",
    "4. Set up ChromaDB vector database\n",
    "5. Store chunks with embeddings and metadata\n",
    "6. Test similarity search functionality\n",
    "\n",
    "## Output\n",
    "- ChromaDB database with embedded chunks\n",
    "- Embedding statistics and performance metrics\n",
    "- Sample similarity search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Embedding and vector database\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks Directory: d:\\AI Book RAG\\data\\chunks\n",
      "ChromaDB Directory: d:\\AI Book RAG\\chroma_db\n",
      "\n",
      "Configuration:\n",
      "  Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Batch Size: 32\n",
      "  Collection Name: ai_books_collection\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration: Set up paths and parameters\n",
    "BASE_DIR = Path(r\"d:\\AI Book RAG\")\n",
    "CHUNKS_DIR = BASE_DIR / \"data\" / \"chunks\"\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "\n",
    "# Create ChromaDB directory\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Embedding model configuration\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "BATCH_SIZE = 32  # Process embeddings in batches for efficiency\n",
    "\n",
    "# ChromaDB collection name\n",
    "COLLECTION_NAME = \"ai_books_collection\"\n",
    "\n",
    "print(f\"Chunks Directory: {CHUNKS_DIR}\")\n",
    "print(f\"ChromaDB Directory: {CHROMA_DIR}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Collection Name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from: d:\\AI Book RAG\\data\\chunks\\all_chunks_semantic.json\n",
      "✓ Loaded 10620 chunks\n",
      "\n",
      "Sample chunk structure:\n",
      "  chunk_id: 299f007b_p2_sc0\n",
      "  global_index: 0\n",
      "  book_title: AI Engineering\n",
      "  chapter: Introduction\n",
      "  page_number: 2\n",
      "  chunk_index: 0\n",
      "  text: [text content - 394 chars]\n",
      "  text_with_context: Drawing on her deep expertise, AI Engineering is a comprehensive and\n",
      "holistic guide to building generative AI applications in production.”\n",
      "Luke Metz, cocreator of ChatGPT, former research manager at OpenAI\n",
      "AI Engineering\n",
      "Foundation models have enabled many new AI use cases while lowering the barriers to entry for\n",
      "building AI products. “This book of fers a comprehensive, well-structured guide to the essential\n",
      "aspects of building generative AI systems. A must-read for any professional\n",
      "looking to scale AI across the enterprise.”\n",
      "Vittorio Cretella, former global CIO at P&G and Mars\n",
      "“Chip Huyen gets generative AI. She is a remarkable teacher and writer\n",
      "whose work has been instrumental in helping teams bring AI into production.\n",
      "  context: Drawing on her deep expertise, AI Engineering is a comprehensive and\n",
      "holistic guide to building generative AI applications in production.”\n",
      "Luke Metz, cocreator of ChatGPT, former research manager at OpenAI\n",
      "AI Engineering\n",
      "Foundation models have enabled many new AI use cases while lowering the barriers to entry for\n",
      "building AI products.\n",
      "  char_count: 394\n",
      "  word_count: 62\n",
      "  sentence_count: 3\n",
      "  citation: [AI Engineering, Introduction, Page 2]\n",
      "  chunking_method: semantic\n"
     ]
    }
   ],
   "source": [
    "# Load chunked data from Notebook 2\n",
    "chunks_file = CHUNKS_DIR / \"all_chunks_semantic.json\"\n",
    "\n",
    "print(f\"Loading chunks from: {chunks_file}\")\n",
    "\n",
    "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "    all_chunks = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(all_chunks)} chunks\")\n",
    "\n",
    "# Display sample chunk structure\n",
    "if all_chunks:\n",
    "    print(\"\\nSample chunk structure:\")\n",
    "    sample_chunk = all_chunks[0]\n",
    "    for key in sample_chunk.keys():\n",
    "        if key != 'text':  # Don't print full text\n",
    "            print(f\"  {key}: {sample_chunk[key]}\")\n",
    "        else:\n",
    "            print(f\"  {key}: [text content - {len(sample_chunk[key])} chars]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "This may take a moment on first run (downloading model)...\n",
      "\n",
      "✓ Model loaded successfully in 3.38 seconds\n",
      "  Model dimension: 384\n",
      "  Max sequence length: 256\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(f\"\\nInitializing embedding model: {EMBEDDING_MODEL}\")\n",
    "print(\"This may take a moment on first run (downloading model)...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the sentence transformer model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Model loaded successfully in {load_time:.2f} seconds\")\n",
    "print(f\"  Model dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max sequence length: {embedding_model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test embedding generation:\n",
      "  Input text: 'What is machine learning and how does it work?'\n",
      "  Embedding shape: (384,)\n",
      "  Embedding type: <class 'numpy.ndarray'>\n",
      "  First 5 values: [-0.03735023  0.01238793  0.0144394   0.01891053  0.03915351]\n"
     ]
    }
   ],
   "source": [
    "# Test embedding generation with a sample\n",
    "sample_text = \"What is machine learning and how does it work?\"\n",
    "sample_embedding = embedding_model.encode(sample_text)\n",
    "\n",
    "print(f\"\\nTest embedding generation:\")\n",
    "print(f\"  Input text: '{sample_text}'\")\n",
    "print(f\"  Embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"  Embedding type: {type(sample_embedding)}\")\n",
    "print(f\"  First 5 values: {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing ChromaDB client...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDB client initialized\n",
      "  Persist directory: d:\\AI Book RAG\\chroma_db\n",
      "  Existing collections: 1\n",
      "    - ai_books_collection\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client\n",
    "print(f\"\\nInitializing ChromaDB client...\")\n",
    "\n",
    "# Create persistent client\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(CHROMA_DIR),\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        allow_reset=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"✓ ChromaDB client initialized\")\n",
    "print(f\"  Persist directory: {CHROMA_DIR}\")\n",
    "\n",
    "# List existing collections (if any)\n",
    "existing_collections = chroma_client.list_collections()\n",
    "print(f\"  Existing collections: {len(existing_collections)}\")\n",
    "for col in existing_collections:\n",
    "    print(f\"    - {col.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up collection: ai_books_collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Deleted existing collection: ai_books_collection\n",
      "✓ Collection created: ai_books_collection\n",
      "  Metadata: {'description': 'AI Books RAG Collection', 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2', 'created_at': '2025-12-30 02:01:03'}\n"
     ]
    }
   ],
   "source": [
    "# Create or get collection\n",
    "print(f\"\\nSetting up collection: {COLLECTION_NAME}\")\n",
    "\n",
    "# Delete existing collection if it exists (for clean start)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=COLLECTION_NAME)\n",
    "    print(f\"  Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    print(f\"  No existing collection to delete\")\n",
    "\n",
    "# Create new collection with custom embedding function\n",
    "collection = chroma_client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\n",
    "        \"description\": \"AI Books RAG Collection\",\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ Collection created: {COLLECTION_NAME}\")\n",
    "print(f\"  Metadata: {collection.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Generate embeddings in batches\n",
    "def generate_embeddings_batch(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = embedding_model.encode(\n",
    "            batch,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for 10620 chunks...\n",
      "This may take several minutes depending on your hardware.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 332/332 [13:44<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Embeddings generated successfully\n",
      "  Total time: 824.56 seconds\n",
      "  Time per chunk: 0.0776 seconds\n",
      "  Embeddings shape: (10620, 384)\n",
      "  Memory usage: 15.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(all_chunks)} chunks...\")\n",
    "print(f\"This may take several minutes depending on your hardware.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract texts from chunks\n",
    "chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = generate_embeddings_batch(chunk_texts, batch_size=BATCH_SIZE)\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Embeddings generated successfully\")\n",
    "print(f\"  Total time: {embedding_time:.2f} seconds\")\n",
    "print(f\"  Time per chunk: {embedding_time / len(all_chunks):.4f} seconds\")\n",
    "print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  Memory usage: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for ChromaDB...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'token_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m ids \u001b[38;5;241m=\u001b[39m [chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n\u001b[0;32m      6\u001b[0m documents \u001b[38;5;241m=\u001b[39m [chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n\u001b[1;32m----> 7\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     {\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchapter\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchapter\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_index\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_count\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m     }\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m all_chunks\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert embeddings to list format for ChromaDB\u001b[39;00m\n\u001b[0;32m     22\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m ids \u001b[38;5;241m=\u001b[39m [chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n\u001b[0;32m      6\u001b[0m documents \u001b[38;5;241m=\u001b[39m [chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n\u001b[0;32m      7\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     {\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchapter\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchapter\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_index\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_count\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m     }\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m all_chunks\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert embeddings to list format for ChromaDB\u001b[39;00m\n\u001b[0;32m     22\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'token_count'"
     ]
    }
   ],
   "source": [
    "# Prepare data for ChromaDB insertion\n",
    "print(f\"\\nPreparing data for ChromaDB...\")\n",
    "\n",
    "# Extract IDs, documents, metadatas, and embeddings\n",
    "ids = [chunk['chunk_id'] for chunk in all_chunks]\n",
    "documents = [chunk['text'] for chunk in all_chunks]\n",
    "\n",
    "# Handle both old (character-based) and new (semantic) chunk formats\n",
    "metadatas = []\n",
    "for chunk in all_chunks:\n",
    "    metadata = {\n",
    "        \"book_title\": chunk['book_title'],\n",
    "        \"chapter\": chunk['chapter'],\n",
    "        \"page_number\": chunk['page_number'],\n",
    "        \"chunk_index\": chunk['chunk_index'],\n",
    "        \"citation\": chunk['citation'],\n",
    "        \"char_count\": chunk['char_count'],\n",
    "        \"word_count\": chunk['word_count']\n",
    "    }\n",
    "    \n",
    "    # Add optional fields if they exist\n",
    "    if 'token_count' in chunk:\n",
    "        metadata['token_count'] = chunk['token_count']\n",
    "    if 'sentence_count' in chunk:\n",
    "        metadata['sentence_count'] = chunk['sentence_count']\n",
    "    if 'chunking_method' in chunk:\n",
    "        metadata['chunking_method'] = chunk['chunking_method']\n",
    "    \n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Convert embeddings to list format for ChromaDB\n",
    "embeddings_list = embeddings.tolist()\n",
    "\n",
    "print(f\"✓ Data prepared\")\n",
    "print(f\"  IDs: {len(ids)}\")\n",
    "print(f\"  Documents: {len(documents)}\")\n",
    "print(f\"  Metadatas: {len(metadatas)}\")\n",
    "print(f\"  Embeddings: {len(embeddings_list)}\")\n",
    "\n",
    "# Show sample metadata\n",
    "if metadatas:\n",
    "    print(f\"\\nSample metadata fields:\")\n",
    "    for key in metadatas[0].keys():\n",
    "        print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to ChromaDB collection in batches\n",
    "print(f\"\\nAdding data to ChromaDB collection...\")\n",
    "print(f\"Processing in batches of {BATCH_SIZE}...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ChromaDB has a limit on batch size, so we insert in chunks\n",
    "for i in tqdm(range(0, len(ids), BATCH_SIZE), desc=\"Inserting into ChromaDB\"):\n",
    "    batch_end = min(i + BATCH_SIZE, len(ids))\n",
    "    \n",
    "    collection.add(\n",
    "        ids=ids[i:batch_end],\n",
    "        documents=documents[i:batch_end],\n",
    "        metadatas=metadatas[i:batch_end],\n",
    "        embeddings=embeddings_list[i:batch_end]\n",
    "    )\n",
    "\n",
    "insert_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Data inserted successfully\")\n",
    "print(f\"  Total time: {insert_time:.2f} seconds\")\n",
    "print(f\"  Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify collection\n",
    "print(f\"\\nVerifying ChromaDB collection...\")\n",
    "\n",
    "collection_count = collection.count()\n",
    "print(f\"✓ Collection verification:\")\n",
    "print(f\"  Name: {collection.name}\")\n",
    "print(f\"  Total documents: {collection_count}\")\n",
    "print(f\"  Metadata: {collection.metadata}\")\n",
    "\n",
    "# Verify data integrity\n",
    "assert collection_count == len(all_chunks), \"Mismatch between chunks and collection count!\"\n",
    "print(f\"\\n✓ Data integrity verified: {collection_count} chunks stored successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search functionality\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TESTING SIMILARITY SEARCH\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is transfer learning?\",\n",
    "    \"Explain the transformer architecture\",\n",
    "    \"How do I train a neural network?\",\n",
    "    \"What are the best practices for fine-tuning LLMs?\"\n",
    "]\n",
    "\n",
    "# Number of results to retrieve\n",
    "top_k = 3\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Book: {metadata['book_title']}\")\n",
    "        print(f\"  Chapter: {metadata['chapter']}\")\n",
    "        print(f\"  Page: {metadata['page_number']}\")\n",
    "        print(f\"  Citation: {metadata['citation']}\")\n",
    "        print(f\"  Distance: {distance:.4f}\")\n",
    "        print(f\"  Text preview: {doc[:200]}...\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test filtering by book\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TESTING FILTERED SEARCH (by book)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Get unique book titles\n",
    "book_titles = list(set(chunk['book_title'] for chunk in all_chunks))\n",
    "test_book = book_titles[0] if book_titles else None\n",
    "\n",
    "if test_book:\n",
    "    query = \"What is machine learning?\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Filter: Book = '{test_book}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Search with filter\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3,\n",
    "        where={\"book_title\": test_book},\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Book: {metadata['book_title']}\")\n",
    "        print(f\"  Chapter: {metadata['chapter']}\")\n",
    "        print(f\"  Page: {metadata['page_number']}\")\n",
    "        print(f\"  Distance: {distance:.4f}\")\n",
    "        print(f\"  Text preview: {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Collection statistics\n",
    "print(f\"ChromaDB Collection:\")\n",
    "print(f\"  Name: {COLLECTION_NAME}\")\n",
    "print(f\"  Total documents: {collection.count():,}\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"  Database size: {sum(f.stat().st_size for f in CHROMA_DIR.rglob('*') if f.is_file()) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Books statistics\n",
    "books_count = {}\n",
    "for chunk in all_chunks:\n",
    "    book = chunk['book_title']\n",
    "    books_count[book] = books_count.get(book, 0) + 1\n",
    "\n",
    "print(f\"\\nBooks in Collection:\")\n",
    "for book, count in sorted(books_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {book}: {count:,} chunks\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Embedding generation: {embedding_time:.2f} seconds\")\n",
    "print(f\"  Database insertion: {insert_time:.2f} seconds\")\n",
    "print(f\"  Total processing time: {embedding_time + insert_time:.2f} seconds\")\n",
    "print(f\"  Average time per chunk: {(embedding_time + insert_time) / len(all_chunks):.4f} seconds\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Vector database setup complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration for later use\n",
    "config = {\n",
    "    \"collection_name\": COLLECTION_NAME,\n",
    "    \"embedding_model\": EMBEDDING_MODEL,\n",
    "    \"embedding_dimension\": int(embeddings.shape[1]),\n",
    "    \"total_chunks\": len(all_chunks),\n",
    "    \"chroma_path\": str(CHROMA_DIR),\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"books\": list(books_count.keys())\n",
    "}\n",
    "\n",
    "config_file = BASE_DIR / \"vectordb_config.json\"\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Configuration saved to: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "✅ Vector database setup complete!\n",
    "\n",
    "The embedded chunks are now stored in ChromaDB and ready for retrieval:\n",
    "- **Notebook 4**: RAG pipeline testing\n",
    "\n",
    "### What Was Created:\n",
    "1. **ChromaDB Collection**: `ai_books_collection` with all embedded chunks\n",
    "2. **Embeddings**: Generated using sentence-transformers model\n",
    "3. **Metadata**: Book title, chapter, page number, citations preserved\n",
    "4. **Configuration**: Saved to `vectordb_config.json`\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Semantic search capability\n",
    "- ✅ Metadata filtering (by book, chapter, etc.)\n",
    "- ✅ Distance-based relevance scoring\n",
    "- ✅ Persistent storage (survives restarts)\n",
    "\n",
    "### What's Next:\n",
    "In the next notebook, we will:\n",
    "1. Test the complete RAG pipeline\n",
    "2. Integrate with Groq LLM\n",
    "3. Generate answers with citations\n",
    "4. Link relevant images to responses\n",
    "5. Evaluate response quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Book RAG)",
   "language": "python",
   "name": "ai-book-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

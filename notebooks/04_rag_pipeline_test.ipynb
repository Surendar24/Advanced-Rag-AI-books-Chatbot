{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: RAG Pipeline Testing\n",
    "\n",
    "## Purpose\n",
    "This notebook tests the complete RAG (Retrieval-Augmented Generation) pipeline by integrating ChromaDB with Groq LLM.\n",
    "\n",
    "## Process\n",
    "1. Load ChromaDB collection from Notebook 3\n",
    "2. Initialize Groq LLM client\n",
    "3. Create RAG pipeline:\n",
    "   - Accept user query\n",
    "   - Retrieve relevant chunks from vector database\n",
    "   - Link relevant images to chunks\n",
    "   - Generate answer using Groq LLM with citations\n",
    "4. Test with various queries\n",
    "5. Evaluate response quality\n",
    "\n",
    "## Output\n",
    "- Working RAG pipeline\n",
    "- Sample Q&A with citations\n",
    "- Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Vector database and embeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LLM\n",
    "from groq import Groq\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Groq API key loaded\n",
      "\n",
      "Configuration:\n",
      "  Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Groq Model: llama-3.3-70b-versatile\n",
      "  Temperature: 0.1\n",
      "  Max Tokens: 2048\n",
      "  Top K Results: 5\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path(r\"d:\\AI Book RAG\")\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "IMAGES_METADATA_FILE = BASE_DIR / \"data\" / \"extracted\" / \"images_metadata.json\"\n",
    "\n",
    "# API Keys\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  WARNING: GROQ_API_KEY not found in .env file\")\n",
    "    print(\"Please add your Groq API key to the .env file\")\n",
    "else:\n",
    "    print(\"‚úì Groq API key loaded\")\n",
    "\n",
    "# Model configuration\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "GROQ_MODEL = os.getenv(\"GROQ_MODEL\", \"mixtral-8x7b-32768\")\n",
    "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.1\"))\n",
    "MAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"2048\"))\n",
    "\n",
    "# RAG configuration\n",
    "TOP_K_RESULTS = int(os.getenv(\"TOP_K_RESULTS\", \"5\"))\n",
    "COLLECTION_NAME = \"ai_books_collection\"\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  Groq Model: {GROQ_MODEL}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Max Tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Top K Results: {TOP_K_RESULTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úì Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(f\"\\nLoading embedding model: {EMBEDDING_MODEL}\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"‚úì Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ChromaDB connected\n",
      "  Collection: ai_books_collection\n",
      "  Total documents: 6,950\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB client and load collection\n",
    "print(f\"\\nConnecting to ChromaDB...\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(CHROMA_DIR),\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "# Get the collection\n",
    "collection = chroma_client.get_collection(name=COLLECTION_NAME)\n",
    "\n",
    "print(f\"‚úì ChromaDB connected\")\n",
    "print(f\"  Collection: {collection.name}\")\n",
    "print(f\"  Total documents: {collection.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading images metadata...\n",
      "‚úì Loaded metadata for 1336 images\n"
     ]
    }
   ],
   "source": [
    "# Load images metadata\n",
    "print(f\"\\nLoading images metadata...\")\n",
    "\n",
    "if IMAGES_METADATA_FILE.exists():\n",
    "    with open(IMAGES_METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        images_metadata = json.load(f)\n",
    "    print(f\"‚úì Loaded metadata for {len(images_metadata)} images\")\n",
    "else:\n",
    "    images_metadata = []\n",
    "    print(f\"‚ö†Ô∏è  No images metadata found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Groq client...\n",
      "‚úì Groq client initialized\n",
      "  Model: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq client\n",
    "print(f\"\\nInitializing Groq client...\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "    print(f\"‚úì Groq client initialized\")\n",
    "    print(f\"  Model: {GROQ_MODEL}\")\n",
    "else:\n",
    "    groq_client = None\n",
    "    print(f\"‚ö†Ô∏è  Groq client not initialized (missing API key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Find relevant images for retrieved chunks\n",
    "def find_relevant_images(\n",
    "    chunks_metadata: List[Dict[str, Any]], \n",
    "    images_metadata: List[Dict[str, Any]],\n",
    "    max_images: int = 3\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Find images that are relevant to the retrieved chunks.\n",
    "    Matches based on book title and page number proximity.\n",
    "    \n",
    "    Args:\n",
    "        chunks_metadata: List of metadata from retrieved chunks\n",
    "        images_metadata: List of all image metadata\n",
    "        max_images: Maximum number of images to return\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant image metadata\n",
    "    \"\"\"\n",
    "    relevant_images = []\n",
    "    \n",
    "    # Get book titles and page numbers from chunks\n",
    "    chunk_pages = {}\n",
    "    for chunk in chunks_metadata:\n",
    "        book = chunk['book_title']\n",
    "        page = chunk['page_number']\n",
    "        if book not in chunk_pages:\n",
    "            chunk_pages[book] = []\n",
    "        chunk_pages[book].append(page)\n",
    "    \n",
    "    # Find images from the same books and nearby pages\n",
    "    for img in images_metadata:\n",
    "        book = img['book_title']\n",
    "        page = img['page_number']\n",
    "        \n",
    "        if book in chunk_pages:\n",
    "            # Check if image is within 2 pages of any chunk\n",
    "            for chunk_page in chunk_pages[book]:\n",
    "                if abs(page - chunk_page) <= 2:\n",
    "                    relevant_images.append(img)\n",
    "                    break\n",
    "        \n",
    "        if len(relevant_images) >= max_images:\n",
    "            break\n",
    "    \n",
    "    return relevant_images[:max_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Create system prompt for Groq\n",
    "def create_system_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Create the system prompt that instructs the LLM on how to respond.\n",
    "    \"\"\"\n",
    "    return \"\"\"You are an AI assistant specialized in Artificial Intelligence and Machine Learning topics.\n",
    "\n",
    "Your role is to answer questions using ONLY the provided book content. Follow these rules strictly:\n",
    "\n",
    "1. ONLY use information from the provided context to answer questions\n",
    "2. Include inline citations in the format: [Book Title, Chapter, Page]\n",
    "3. If relevant diagrams or images are mentioned in the context, reference them in your answer\n",
    "4. If the context doesn't contain enough information to answer the question, say so clearly\n",
    "5. Do NOT generate information outside the given content\n",
    "6. Be concise but comprehensive\n",
    "7. Use technical terminology appropriately\n",
    "8. When multiple sources discuss the same topic, synthesize the information and cite all sources\n",
    "\n",
    "Format your citations like this:\n",
    "- Single source: \"Transfer learning is a technique... [Hands-On Machine Learning, Chapter 11, Page 342]\"\n",
    "- Multiple sources: \"Neural networks consist of layers... [AI Engineering, Chapter 3, Page 45] [Deep Learning Book, Chapter 6, Page 167]\"\n",
    "\n",
    "If diagrams are relevant, mention them like:\n",
    "\"See Figure X showing the transformer architecture [Book Title, Chapter Y, Page Z]\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Format context from retrieved chunks\n",
    "def format_context(chunks: List[str], metadatas: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieved chunks into context for the LLM.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of text chunks\n",
    "        metadatas: List of metadata for each chunk\n",
    "    \n",
    "    Returns:\n",
    "        Formatted context string\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, (chunk, metadata) in enumerate(zip(chunks, metadatas), 1):\n",
    "        context_part = f\"\"\"Source {i}:\n",
    "Book: {metadata['book_title']}\n",
    "Chapter: {metadata['chapter']}\n",
    "Page: {metadata['page_number']}\n",
    "Citation: {metadata['citation']}\n",
    "\n",
    "Content:\n",
    "{chunk}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "        context_parts.append(context_part)\n",
    "    \n",
    "    return \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main RAG function\n",
    "def rag_query(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K_RESULTS,\n",
    "    include_images: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute the complete RAG pipeline for a query.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of chunks to retrieve\n",
    "        include_images: Whether to find relevant images\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing answer, sources, and images\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nProcessing query: '{query}'\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Step 1: Generate query embedding\n",
    "    if verbose:\n",
    "        print(\"1. Generating query embedding...\")\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    \n",
    "    # Step 2: Retrieve relevant chunks from ChromaDB\n",
    "    if verbose:\n",
    "        print(f\"2. Retrieving top {top_k} relevant chunks...\")\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    chunks = results['documents'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "    distances = results['distances'][0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Retrieved {len(chunks)} chunks\")\n",
    "    \n",
    "    # Step 3: Find relevant images\n",
    "    relevant_images = []\n",
    "    if include_images and images_metadata:\n",
    "        if verbose:\n",
    "            print(\"3. Finding relevant images...\")\n",
    "        relevant_images = find_relevant_images(metadatas, images_metadata)\n",
    "        if verbose:\n",
    "            print(f\"   Found {len(relevant_images)} relevant images\")\n",
    "    \n",
    "    # Step 4: Format context for LLM\n",
    "    if verbose:\n",
    "        print(\"4. Formatting context...\")\n",
    "    context = format_context(chunks, metadatas)\n",
    "    \n",
    "    # Step 5: Generate answer with Groq\n",
    "    if verbose:\n",
    "        print(\"5. Generating answer with Groq LLM...\")\n",
    "    \n",
    "    if not groq_client:\n",
    "        answer = \"Error: Groq client not initialized. Please add GROQ_API_KEY to .env file.\"\n",
    "        llm_time = 0\n",
    "    else:\n",
    "        llm_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = groq_client.chat.completions.create(\n",
    "                model=GROQ_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": create_system_prompt()},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Context from AI books:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based on the context above, including all relevant citations.\"\"\"}\n",
    "                ],\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_TOKENS\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            llm_time = time.time() - llm_start\n",
    "            \n",
    "        except Exception as e:\n",
    "            answer = f\"Error generating answer: {str(e)}\"\n",
    "            llm_time = 0\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚úì Query processed in {total_time:.2f} seconds\")\n",
    "        print(f\"  - LLM generation: {llm_time:.2f} seconds\")\n",
    "    \n",
    "    # Return complete result\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": metadata,\n",
    "                \"distance\": distance\n",
    "            }\n",
    "            for chunk, metadata, distance in zip(chunks, metadatas, distances)\n",
    "        ],\n",
    "        \"images\": relevant_images,\n",
    "        \"metrics\": {\n",
    "            \"total_time\": total_time,\n",
    "            \"llm_time\": llm_time,\n",
    "            \"num_sources\": len(chunks),\n",
    "            \"num_images\": len(relevant_images)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Display RAG result\n",
    "def display_result(result: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Display the RAG query result in a formatted way.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUERY RESULT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìù Query: {result['query']}\")\n",
    "    \n",
    "    print(f\"\\nüí° Answer:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result['answer'])\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nüìö Sources ({len(result['sources'])}):\")\n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        metadata = source['metadata']\n",
    "        print(f\"\\n  [{i}] {metadata['citation']}\")\n",
    "        print(f\"      Distance: {source['distance']:.4f}\")\n",
    "        print(f\"      Preview: {source['text'][:150]}...\")\n",
    "    \n",
    "    if result['images']:\n",
    "        print(f\"\\nüñºÔ∏è  Relevant Images ({len(result['images'])}):\")\n",
    "        for i, img in enumerate(result['images'], 1):\n",
    "            print(f\"\\n  [{i}] {img['filename']}\")\n",
    "            print(f\"      Book: {img['book_title']}\")\n",
    "            print(f\"      Page: {img['page_number']}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Metrics:\")\n",
    "    print(f\"  Total time: {result['metrics']['total_time']:.2f}s\")\n",
    "    print(f\"  LLM time: {result['metrics']['llm_time']:.2f}s\")\n",
    "    print(f\"  Sources retrieved: {result['metrics']['num_sources']}\")\n",
    "    print(f\"  Images found: {result['metrics']['num_images']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: 'What is transfer learning and how does it work in deep learning?'\n",
      "--------------------------------------------------------------------------------\n",
      "1. Generating query embedding...\n",
      "2. Retrieving top 5 relevant chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Retrieved 5 chunks\n",
      "3. Finding relevant images...\n",
      "   Found 3 relevant images\n",
      "4. Formatting context...\n",
      "5. Generating answer with Groq LLM...\n",
      "\n",
      "‚úì Query processed in 2.93 seconds\n",
      "  - LLM generation: 2.18 seconds\n",
      "\n",
      "================================================================================\n",
      "QUERY RESULT\n",
      "================================================================================\n",
      "\n",
      "üìù Query: What is transfer learning and how does it work in deep learning?\n",
      "\n",
      "üí° Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "Transfer learning is a technique in deep learning where a model trained on one task is used as a starting point for a new, but related task [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403]. It uses the intelligence already present in the bottleneck layers of pretrained models to extract features, and then uses its own classification layers to interpret the results [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403]. This approach is particularly useful when there is a limited amount of training data available for the new task [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 378].\n",
      "\n",
      "In the context of computer vision, transfer learning involves using a convolutional neural network (CNN) trained on one task, such as image recognition, and fine-tuning it on a new task, such as object detection [NLP with Transformer models, Chapter 3: , but, Page 30]. This allows the network to make use of the knowledge learned from the original task and adapt to the new task [NLP with Transformer models, Chapter 3: , but, Page 30]. Architecturally, this involves splitting the model into a body and a head, where the head is a task-specific network [NLP with Transformer models, Chapter 3: , but, Page 30].\n",
      "\n",
      "Transfer learning has been successfully applied in various domains, including image recognition [LLM Engineers Handbook, Chapter 4: 173, Page 203] and natural language processing (NLP) [NLP with Transformer models, Chapter 3: , but, Page 30]. In NLP, transfer learning has paved the way for many recent breakthroughs, including the development of transformer models [NLP with Transformer models, Chapter 3: , but, Page 30]. The original transformer paper demonstrated the effectiveness of transfer learning in NLP by training a translation model from scratch on a large corpus of sentence pairs and then fine-tuning it on a new task [NLP with Transformer models, Chapter 3: , but, Page 30].\n",
      "\n",
      "Data augmentation is also an important aspect of transfer learning, as it can increase the accuracy of a model trained with a relatively small number of images [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403]. Data augmentation involves applying random transforms, such as translations and rotations, to the training images [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403].\n",
      "\n",
      "In summary, transfer learning is a powerful technique in deep learning that allows models to leverage knowledge learned from one task and apply it to a new, but related task [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403] [NLP with Transformer models, Chapter 3: , but, Page 30]. It has been successfully applied in various domains, including computer vision and NLP, and is a key component of many state-of-the-art models [LLM Engineers Handbook, Chapter 4: 173, Page 203]. \n",
      "\n",
      "References:\n",
      "[Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403]\n",
      "[Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 378]\n",
      "[NLP with Transformer models, Chapter 3: , but, Page 30]\n",
      "[LLM Engineers Handbook, Chapter 4: 173, Page 203]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìö Sources (5):\n",
      "\n",
      "  [1] [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 403]\n",
      "      Distance: 0.6786\n",
      "      Preview: fully connected layers for classification. Keras provides\n",
      "implementations of convolution and pooling layers in classes\n",
      "such as and .\n",
      "Conv2D MaxPooling...\n",
      "\n",
      "  [2] [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 378]\n",
      "      Distance: 0.7206\n",
      "      Preview: That‚Äôs not to say that transfer learning will always get you\n",
      "97% accuracy with 100 images per class. It won‚Äôt. If a\n",
      "dataset lacks the information to a...\n",
      "\n",
      "  [3] [Applied-Machine-Learning-and-AI-for-Engineers, Chapter 13: to increase the accuracy of an NLP model that utilizes, Page 371]\n",
      "      Distance: 0.7975\n",
      "      Preview: predictions = model.predict(features)\n",
      "And with that, transfer learning is complete. All that\n",
      "remains is to put it in practice.\n",
      "Using Transfer Learning...\n",
      "\n",
      "  [4] [LLM Engineers Handbook, Chapter 4: 173, Page 203]\n",
      "      Distance: 0.8009\n",
      "      Preview: ‚Ä¢ He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In\n",
      "Proceedings of the IEEE conference on computer vision...\n",
      "\n",
      "  [5] [NLP with Transformer models, Chapter 3: , but, Page 30]\n",
      "      Distance: 0.8014\n",
      "      Preview: and paved the way for many of the recent breakthroughs in NLP.\n",
      "Figure 1-6. Encoder-decoder architecture of the original Transformer\n",
      "In the original Tr...\n",
      "\n",
      "üñºÔ∏è  Relevant Images (3):\n",
      "\n",
      "  [1] 5ec0620f_p379_img0.jpeg\n",
      "      Book: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "      Page: 379\n",
      "\n",
      "  [2] 5ec0620f_p380_img0.jpeg\n",
      "      Book: Applied-Machine-Learning-and-AI-for-Engineers\n",
      "      Page: 380\n",
      "\n",
      "  [3] 8d2306ab_p28_img0.png\n",
      "      Book: NLP with Transformer models\n",
      "      Page: 28\n",
      "\n",
      "‚è±Ô∏è  Metrics:\n",
      "  Total time: 2.93s\n",
      "  LLM time: 2.18s\n",
      "  Sources retrieved: 5\n",
      "  Images found: 3\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 1: Transfer Learning\n",
    "query1 = \"What is transfer learning and how does it work in deep learning?\"\n",
    "result1 = rag_query(query1)\n",
    "display_result(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: 'Explain the transformer architecture and its key components'\n",
      "--------------------------------------------------------------------------------\n",
      "1. Generating query embedding...\n",
      "2. Retrieving top 5 relevant chunks...\n",
      "   Retrieved 5 chunks\n",
      "3. Finding relevant images...\n",
      "   Found 3 relevant images\n",
      "4. Formatting context...\n",
      "5. Generating answer with Groq LLM...\n",
      "\n",
      "‚úì Query processed in 1.26 seconds\n",
      "  - LLM generation: 1.24 seconds\n",
      "\n",
      "================================================================================\n",
      "QUERY RESULT\n",
      "================================================================================\n",
      "\n",
      "üìù Query: Explain the transformer architecture and its key components\n",
      "\n",
      "üí° Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "The transformer architecture is a widely used model in natural language processing (NLP) tasks, such as machine translation [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81]. The original Transformer architecture is based on the encoder-decoder architecture, which consists of two main components: the encoder and the decoder [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81]. The encoder converts an input sequence of tokens into a sequence of embedding vectors, often called the hidden state or context [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81].\n",
      "\n",
      "The key components of the transformer architecture include the self-attention layer, which is the most important building block [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 84]. The self-attention layer combines the relevant information of previous positions by multiplying their relevance scores by their respective value vectors, as shown in Figure 3-21 [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 117]. The self-attention layer is a crucial component of the transformer architecture, as it allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
      "\n",
      "In addition to the self-attention layer, the transformer architecture also uses skip connections and layer normalization, which are standard tricks to train deep neural networks effectively [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 84]. The transformer architecture has undergone significant improvements since its release, with recent developments focusing on optimizing the training process, learning rates, and architectural modifications [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 117].\n",
      "\n",
      "Overall, the transformer architecture is a powerful model that has revolutionized the field of NLP, and its key components, including the self-attention layer, encoder, and decoder, work together to enable the model to learn complex patterns and relationships in language data [NLP with Transformer models, Introduction, Page 3] [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81] [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 117].\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìö Sources (5):\n",
      "\n",
      "  [1] [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 117]\n",
      "      Distance: 0.6129\n",
      "      Preview: Figure 3-21. Attention combines the relevant information of previous positions by multi‚Äê\n",
      "plying their relevance scores by their respective value vecto...\n",
      "\n",
      "  [2] [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81]\n",
      "      Distance: 0.7074\n",
      "      Preview: CHAPTER 3\n",
      "Transformer Anatomy\n",
      "In Chapter 2, we saw what it takes to fine-tune and evaluate a transformer. Now let‚Äôs\n",
      "take a look at how they work under...\n",
      "\n",
      "  [3] [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 81]\n",
      "      Distance: 0.7092\n",
      "      Preview: not necessary to use Transformers and fine-tune models for your use case, it can\n",
      "be helpful for comprehending and navigating the limitations of transf...\n",
      "\n",
      "  [4] [NLP with Transformer models, Introduction, Page 3]\n",
      "      Distance: 0.7223\n",
      "      Preview: chapter provides a nuanced overview grounded in rich code examples that highlights best\n",
      "practices as well as practical considerations and enables you ...\n",
      "\n",
      "  [5] [NLP with Transformer models, Chapter 3: Transformer Anatomy, Page 84]\n",
      "      Distance: 0.7601\n",
      "      Preview: Each of these sublayers also uses skip connections and layer normalization, which are\n",
      "standard tricks to train deep neural networks effectively. But t...\n",
      "\n",
      "üñºÔ∏è  Relevant Images (3):\n",
      "\n",
      "  [1] b3559b20_p115_img0.png\n",
      "      Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "      Page: 115\n",
      "\n",
      "  [2] b3559b20_p116_img0.png\n",
      "      Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "      Page: 116\n",
      "\n",
      "  [3] b3559b20_p117_img0.png\n",
      "      Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "      Page: 117\n",
      "\n",
      "‚è±Ô∏è  Metrics:\n",
      "  Total time: 1.26s\n",
      "  LLM time: 1.24s\n",
      "  Sources retrieved: 5\n",
      "  Images found: 3\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 2: Transformer Architecture\n",
    "query2 = \"Explain the transformer architecture and its key components\"\n",
    "result2 = rag_query(query2)\n",
    "display_result(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: 'What are the best practices for fine-tuning large language models?'\n",
      "--------------------------------------------------------------------------------\n",
      "1. Generating query embedding...\n",
      "2. Retrieving top 5 relevant chunks...\n",
      "   Retrieved 5 chunks\n",
      "3. Finding relevant images...\n",
      "   Found 3 relevant images\n",
      "4. Formatting context...\n",
      "5. Generating answer with Groq LLM...\n",
      "\n",
      "‚úì Query processed in 1.62 seconds\n",
      "  - LLM generation: 1.60 seconds\n",
      "\n",
      "================================================================================\n",
      "QUERY RESULT\n",
      "================================================================================\n",
      "\n",
      "üìù Query: What are the best practices for fine-tuning large language models?\n",
      "\n",
      "üí° Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "Fine-tuning large language models is a crucial step in adapting these models to specific tasks and datasets [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 384]. According to the context, several best practices can be identified:\n",
      "\n",
      "1. **Supervised Fine-Tuning (SFT)**: This approach involves fine-tuning a pre-trained language model on a specific task with labeled data [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 383]. Research papers such as \"AdapterHub: A framework for adapting transformers\" [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 383] and \"Llama-adapter: Efficient fine-tuning of language models with zero-init attention\" [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 383] provide insights into this approach.\n",
      "2. **Fine-Tuning BERT for Classification**: Fine-tuning BERT for classification tasks is a common practice, and Chapter 11 of the book \"Hands-On Large Language Models Language Understanding and Generation\" provides guidance on this topic [Hands-On Large Language Models Language Understanding and Generation, Chapter 10: ), review how to fine-tune BERT for classification, Page 16].\n",
      "3. **Using Pre-Trained Models**: Using pre-trained models such as BERT as a starting point for fine-tuning can be beneficial [NLP with Transformer models, Chapter 9: Dealing with Few to No Labels, Page 308]. This approach can save computational resources and improve performance.\n",
      "4. **Access to Labeled Data**: Having access to labeled data is essential for fine-tuning large language models [NLP with Transformer models, Chapter 9: Dealing with Few to No Labels, Page 308]. If labeled data is not available, other approaches such as unsupervised or semi-supervised learning may be necessary.\n",
      "5. **Computational Resources**: Fine-tuning large language models requires significant computational resources, including strong GPUs [Hands-On Large Language Models Language Understanding and Generation, Chapter 10: ), review how to fine-tune BERT for classification, Page 16]. Using online platforms such as Google Colab can provide access to these resources.\n",
      "\n",
      "In summary, the best practices for fine-tuning large language models include supervised fine-tuning, fine-tuning BERT for classification, using pre-trained models, having access to labeled data, and utilizing sufficient computational resources [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 384] [NLP with Transformer models, Chapter 9: Dealing with Few to No Labels, Page 308] [Hands-On Large Language Models Language Understanding and Generation, Chapter 10: ), review how to fine-tune BERT for classification, Page 16].\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìö Sources (5):\n",
      "\n",
      "  [1] [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 384]\n",
      "      Distance: 0.6381\n",
      "      Preview: version of these.\n",
      "4 Edward J. Hu et al. ‚ÄúLoR: Low-Rank Adaptation of large language models.‚Äù arXiv preprint arXiv:2106.09685\n",
      "(2021).\n",
      "362 | Chapter 12:...\n",
      "\n",
      "  [2] [Hands-On Large Language Models Language Understanding and Generation, Chapter 10: ), review how to fine-tune BERT for classification, Page 16]\n",
      "      Distance: 0.6945\n",
      "      Preview: Learning these individual language model capabilities will equip you with the skill\n",
      "set to problem-solve with LLMs and build more and more advanced sy...\n",
      "\n",
      "  [3] [NLP with Transformer models, Chapter 9: Dealing with Few to No Labels, Page 308]\n",
      "      Distance: 0.6966\n",
      "      Preview: ferent languages. This enormous corpus was subsequently used to train M2M100, a\n",
      "large machine translation model that is able to directly translate bet...\n",
      "\n",
      "  [4] [Hands-On Large Language Models Language Understanding and Generation, Introduction, Page 3]\n",
      "      Distance: 0.7057\n",
      "      Preview: Praise for Hands-On Large Language Models\n",
      "This is an exceptional guide to the world of language models and their practical\n",
      "applications in industry. I...\n",
      "\n",
      "  [5] [Hands-On Large Language Models Language Understanding and Generation, Chapter 12: Fine-Tuning Generation Models, Page 383]\n",
      "      Distance: 0.7122\n",
      "      Preview: 2 Jonas Pfeiffer et al. ‚ÄúAdapterHub: A framework for adapting transformers.‚Äù arXiv preprint arXiv:2007.07779\n",
      "(2020).\n",
      "3 Renrui Zhang et al. ‚ÄúLlama-adap...\n",
      "\n",
      "üñºÔ∏è  Relevant Images (3):\n",
      "\n",
      "  [1] b3559b20_p1_img0.jpeg\n",
      "      Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "      Page: 1\n",
      "\n",
      "  [2] b3559b20_p15_img0.png\n",
      "      Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "      Page: 15\n",
      "\n",
      "  [3] b3559b20_p16_img0.png\n",
      "      Book: Hands-On Large Language Models Language Understanding and Generation\n",
      "      Page: 16\n",
      "\n",
      "‚è±Ô∏è  Metrics:\n",
      "  Total time: 1.62s\n",
      "  LLM time: 1.60s\n",
      "  Sources retrieved: 5\n",
      "  Images found: 3\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 3: Fine-tuning LLMs\n",
    "query3 = \"What are the best practices for fine-tuning large language models?\"\n",
    "result3 = rag_query(query3)\n",
    "display_result(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: 'How do I train a neural network with PyTorch?'\n",
      "--------------------------------------------------------------------------------\n",
      "1. Generating query embedding...\n",
      "2. Retrieving top 5 relevant chunks...\n",
      "   Retrieved 5 chunks\n",
      "3. Finding relevant images...\n",
      "   Found 0 relevant images\n",
      "4. Formatting context...\n",
      "5. Generating answer with Groq LLM...\n",
      "\n",
      "‚úì Query processed in 1.41 seconds\n",
      "  - LLM generation: 1.39 seconds\n",
      "\n",
      "================================================================================\n",
      "QUERY RESULT\n",
      "================================================================================\n",
      "\n",
      "üìù Query: How do I train a neural network with PyTorch?\n",
      "\n",
      "üí° Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "Training a neural network with PyTorch involves several steps, including building the network, defining the loss function and optimizer, and iterating over the training data [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 10: . Building Neural, Page 488]. \n",
      "\n",
      "To start, you need to get familiar with the core building blocks of PyTorch, namely tensors and autograd [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 10: . Building Neural, Page 489]. You can then build and train a simple linear regression model, and upgrade it to a multilayer neural network for regression or classification.\n",
      "\n",
      "PyTorch provides a higher-level API to simplify the process of building and training neural networks [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 4: , except we will use, Page 502]. You can use the `torch.nn` module to define your neural network architecture, and the `torch.optim` module to define the optimizer and loss function.\n",
      "\n",
      "For example, you can use the `nn.Linear` class to define a linear regression model [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 4: , except we will use, Page 502]. You can then use the `torch.compile()` function to optimize your model for better performance [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 2: . However, you will usually get better results by using a dedicated fine-, Page 535].\n",
      "\n",
      "Additionally, PyTorch provides various tools and techniques to fine-tune and optimize your neural networks, including the Optuna library for hyperparameter tuning [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 10: . Building Neural, Page 489].\n",
      "\n",
      "In summary, training a neural network with PyTorch involves building the network, defining the loss function and optimizer, and iterating over the training data, using PyTorch's higher-level API and various tools and techniques for optimization [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 10: . Building Neural, Page 488] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 4: , except we will use, Page 502] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 2: . However, you will usually get better results by using a dedicated fine-, Page 535].\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìö Sources (5):\n",
      "\n",
      "  [1] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 10: . Building Neural, Page 488]\n",
      "      Distance: 0.5487\n",
      "      Preview: Chapter 10. Building Neural\n",
      "Networks with PyTorch\n",
      "A NOTE FOR EARLY RELEASE READERS\n",
      "With Early Release ebooks, you get books in their earliest form‚Äîthe...\n",
      "\n",
      "  [2] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 2: . However, you will usually get better results by using a dedicated fine-, Page 535]\n",
      "      Distance: 0.6038\n",
      "      Preview: architectures: convolutional neural networks for image processing, recurrent neural\n",
      "networks for sequential data, transformers for text (and much more...\n",
      "\n",
      "  [3] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 10: . Building Neural, Page 489]\n",
      "      Distance: 0.6829\n",
      "      Preview: In this chapter, you will learn how to train, evaluate, fine-tune, optimize, and save\n",
      "neural nets with PyTorch. We will start by getting familiar with...\n",
      "\n",
      "  [4] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 4: , except we will use, Page 502]\n",
      "      Distance: 0.6919\n",
      "      Preview: Implementing linear regression using PyTorch‚Äôs low-level API wasn‚Äôt too hard, but\n",
      "using this approach for more complex models would get really messy a...\n",
      "\n",
      "  [5] [Hands-On Machine Learning with Scikit-Learn and PyTorch (Second Early Release), Chapter 2: . However, you will usually get better results by using a dedicated fine-, Page 535]\n",
      "      Distance: 0.7180\n",
      "      Preview: information that can be used to better optimize the model. The actual compilation and\n",
      "optimization is performed by default by a backend component name...\n",
      "\n",
      "‚è±Ô∏è  Metrics:\n",
      "  Total time: 1.41s\n",
      "  LLM time: 1.39s\n",
      "  Sources retrieved: 5\n",
      "  Images found: 0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 4: Neural Network Training\n",
    "query4 = \"How do I train a neural network with PyTorch?\"\n",
    "result4 = rag_query(query4)\n",
    "display_result(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: 'What is the attention mechanism in neural networks?'\n",
      "--------------------------------------------------------------------------------\n",
      "1. Generating query embedding...\n",
      "2. Retrieving top 5 relevant chunks...\n",
      "   Retrieved 5 chunks\n",
      "3. Finding relevant images...\n",
      "   Found 3 relevant images\n",
      "4. Formatting context...\n",
      "5. Generating answer with Groq LLM...\n",
      "\n",
      "‚úì Query processed in 1.52 seconds\n",
      "  - LLM generation: 1.49 seconds\n",
      "\n",
      "================================================================================\n",
      "QUERY RESULT\n",
      "================================================================================\n",
      "\n",
      "üìù Query: What is the attention mechanism in neural networks?\n",
      "\n",
      "üí° Answer:\n",
      "--------------------------------------------------------------------------------\n",
      "The attention mechanism in neural networks is a key component that allows the model to incorporate contextual information to better capture the nuance of language [Hands-On Large Language Models Language Understanding and Generation, Chapter 3: Looking Inside Large Language Models, Page 128]. It is a major component of Transformer blocks, which are made up of two components: a feedforward neural network and an attention layer [Hands-On Large Language Models Language Understanding and Generation, Chapter 3: Looking Inside Large Language Models, Page 128].\n",
      "\n",
      "The attention mechanism operates in two major steps: (1) scoring relevance and (2) combining information [Hands-On Large Language Models Language Understanding and Generation, Chapter 3: Looking Inside Large Language Models, Page 128]. This process allows the model to assign a different amount of weight, or \"attention,\" to each of the encoder states at every decoding timestep [NLP with Transformer models, Chapter 1: Hello Transformers, Page 28].\n",
      "\n",
      "The attention mechanism leverages key, value, and query vectors: the query vector (Q) represents the current state of the decoder, the key vector (K) represents a previous token, and the value vector is used to compute the weighted sum of the key vectors [AI Engineering, Chapter 1: , a model‚Äôs training process is often divided into pre-, Page 84]. This process is illustrated in Figure 3-14, which shows how the self-attention layer incorporates relevant information from previous positions to help process the current token [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 111].\n",
      "\n",
      "The attention mechanism is essential in language models, as it allows the model to incorporate context as it's processing a specific token [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 110]. For example, in the prompt \"The dog chased the squirrel because it\", the attention mechanism helps the model determine what \"it\" refers to, by adding information from the context into the representation of the \"it\" token [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 110].\n",
      "\n",
      "In summary, the attention mechanism is a crucial component of neural networks, particularly in Transformer models, that allows the model to incorporate contextual information and assign weights to different parts of the input sequence [NLP with Transformer models, Chapter 1: Hello Transformers, Page 28] [Hands-On Large Language Models Language Understanding and Generation, Chapter 3: Looking Inside Large Language Models, Page 128] [AI Engineering, Chapter 1: , a model‚Äôs training process is often divided into pre-, Page 84]. See Figure 3-14 and Figure 1-4 for illustrations of the attention mechanism [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 111] [NLP with Transformer models, Chapter 1: Hello Transformers, Page 28].\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìö Sources (5):\n",
      "\n",
      "  [1] [Hands-On Large Language Models Language Understanding and Generation, Chapter 3: Looking Inside Large Language Models, Page 128]\n",
      "      Distance: 0.7290\n",
      "      Preview: processing effort (these results are stored as various matrices within the layers).\n",
      "‚Ä¢‚Ä¢ The majority of processing happens within Transformer blocks. T...\n",
      "\n",
      "  [2] [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 111]\n",
      "      Distance: 0.7674\n",
      "      Preview: Figure 3-14. The self-attention layer incorporates relevant information from previous\n",
      "positions that help process the current token.\n",
      "The model does th...\n",
      "\n",
      "  [3] [NLP with Transformer models, Chapter 1: Hello Transformers, Page 28]\n",
      "      Distance: 0.7729\n",
      "      Preview: attention,6 and it is a key component in many modern neural network architectures.\n",
      "Understanding how attention was developed for RNNs will put us in g...\n",
      "\n",
      "  [4] [AI Engineering, Chapter 1: , a model‚Äôs training process is often divided into pre-, Page 84]\n",
      "      Distance: 0.7830\n",
      "      Preview: former models work. Under the hood, the attention mechanism leverages key, value,\n",
      "and query vectors:\n",
      "‚Ä¢ The query vector (Q) represents the current sta...\n",
      "\n",
      "  [5] [Hands-On Large Language Models Language Understanding and Generation, Chapter 6: ., Page 110]\n",
      "      Distance: 0.7960\n",
      "      Preview: interpolation based on the previous token can only take us so far. We know that\n",
      "because this was one of the leading approaches to build language model...\n",
      "\n",
      "üñºÔ∏è  Relevant Images (3):\n",
      "\n",
      "  [1] 299f007b_p83_img0.png\n",
      "      Book: AI Engineering\n",
      "      Page: 83\n",
      "\n",
      "  [2] 299f007b_p84_img0.png\n",
      "      Book: AI Engineering\n",
      "      Page: 84\n",
      "\n",
      "  [3] 299f007b_p85_img0.png\n",
      "      Book: AI Engineering\n",
      "      Page: 85\n",
      "\n",
      "‚è±Ô∏è  Metrics:\n",
      "  Total time: 1.52s\n",
      "  LLM time: 1.49s\n",
      "  Sources retrieved: 5\n",
      "  Images found: 3\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Query 5: Attention Mechanism\n",
    "query5 = \"What is the attention mechanism in neural networks?\"\n",
    "result5 = rag_query(query5)\n",
    "display_result(result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch testing with multiple queries\n",
    "test_queries = [\n",
    "    \"What is gradient descent?\",\n",
    "    \"Explain backpropagation in neural networks\",\n",
    "    \"What are embeddings in NLP?\",\n",
    "    \"How does BERT work?\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_results = []\n",
    "for query in test_queries:\n",
    "    result = rag_query(query, verbose=False)\n",
    "    batch_results.append(result)\n",
    "    print(f\"\\n‚úì Processed: {query}\")\n",
    "    print(f\"  Time: {result['metrics']['total_time']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH TESTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "all_results = [result1, result2, result3, result4, result5] + batch_results\n",
    "\n",
    "# Calculate statistics\n",
    "total_queries = len(all_results)\n",
    "avg_total_time = sum(r['metrics']['total_time'] for r in all_results) / total_queries\n",
    "avg_llm_time = sum(r['metrics']['llm_time'] for r in all_results) / total_queries\n",
    "avg_sources = sum(r['metrics']['num_sources'] for r in all_results) / total_queries\n",
    "avg_images = sum(r['metrics']['num_images'] for r in all_results) / total_queries\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal queries tested: {total_queries}\")\n",
    "print(f\"\\nAverage Metrics:\")\n",
    "print(f\"  Total time per query: {avg_total_time:.2f}s\")\n",
    "print(f\"  LLM time per query: {avg_llm_time:.2f}s\")\n",
    "print(f\"  Retrieval time per query: {avg_total_time - avg_llm_time:.2f}s\")\n",
    "print(f\"  Sources per query: {avg_sources:.1f}\")\n",
    "print(f\"  Images per query: {avg_images:.1f}\")\n",
    "\n",
    "# Create performance DataFrame\n",
    "perf_data = []\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    perf_data.append({\n",
    "        'Query #': i,\n",
    "        'Query': result['query'][:50] + '...' if len(result['query']) > 50 else result['query'],\n",
    "        'Total Time (s)': f\"{result['metrics']['total_time']:.2f}\",\n",
    "        'LLM Time (s)': f\"{result['metrics']['llm_time']:.2f}\",\n",
    "        'Sources': result['metrics']['num_sources'],\n",
    "        'Images': result['metrics']['num_images']\n",
    "    })\n",
    "\n",
    "df_perf = pd.DataFrame(perf_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED PERFORMANCE TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(df_perf.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract times\n",
    "total_times = [r['metrics']['total_time'] for r in all_results]\n",
    "llm_times = [r['metrics']['llm_time'] for r in all_results]\n",
    "retrieval_times = [t - l for t, l in zip(total_times, llm_times)]\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Time breakdown\n",
    "x = np.arange(len(all_results))\n",
    "axes[0, 0].bar(x, retrieval_times, label='Retrieval', alpha=0.7)\n",
    "axes[0, 0].bar(x, llm_times, bottom=retrieval_times, label='LLM Generation', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Query Number')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "axes[0, 0].set_title('Time Breakdown per Query')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Average time distribution\n",
    "avg_retrieval = np.mean(retrieval_times)\n",
    "avg_llm = np.mean(llm_times)\n",
    "axes[0, 1].pie([avg_retrieval, avg_llm], labels=['Retrieval', 'LLM'], autopct='%1.1f%%')\n",
    "axes[0, 1].set_title('Average Time Distribution')\n",
    "\n",
    "# Plot 3: Sources retrieved\n",
    "sources_count = [r['metrics']['num_sources'] for r in all_results]\n",
    "axes[1, 0].bar(x, sources_count, alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('Query Number')\n",
    "axes[1, 0].set_ylabel('Number of Sources')\n",
    "axes[1, 0].set_title('Sources Retrieved per Query')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Total time histogram\n",
    "axes[1, 1].hist(total_times, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[1, 1].axvline(x=np.mean(total_times), color='red', linestyle='--', label=f'Mean: {np.mean(total_times):.2f}s')\n",
    "axes[1, 1].set_xlabel('Total Time (seconds)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Query Times')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Performance visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results for documentation\n",
    "results_file = BASE_DIR / \"rag_test_results.json\"\n",
    "\n",
    "# Prepare results for saving (remove large text fields)\n",
    "results_summary = []\n",
    "for result in all_results:\n",
    "    results_summary.append({\n",
    "        \"query\": result['query'],\n",
    "        \"answer_preview\": result['answer'][:200] + \"...\",\n",
    "        \"num_sources\": len(result['sources']),\n",
    "        \"num_images\": len(result['images']),\n",
    "        \"metrics\": result['metrics']\n",
    "    })\n",
    "\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        \"test_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"total_queries\": total_queries,\n",
    "        \"average_metrics\": {\n",
    "            \"total_time\": avg_total_time,\n",
    "            \"llm_time\": avg_llm_time,\n",
    "            \"sources\": avg_sources,\n",
    "            \"images\": avg_images\n",
    "        },\n",
    "        \"results\": results_summary\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Test results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ RAG Pipeline Testing Complete!\n",
    "\n",
    "### What Was Tested:\n",
    "1. **Query Processing**: Embedding generation and similarity search\n",
    "2. **Retrieval**: Top-K relevant chunks from ChromaDB\n",
    "3. **Image Linking**: Finding relevant diagrams based on context\n",
    "4. **Answer Generation**: Groq LLM with citations\n",
    "5. **Performance**: Response times and metrics\n",
    "\n",
    "### Key Features Verified:\n",
    "- ‚úÖ Accurate retrieval of relevant content\n",
    "- ‚úÖ Inline citations in answers\n",
    "- ‚úÖ Image/diagram linking\n",
    "- ‚úÖ Multi-book reasoning\n",
    "- ‚úÖ Fast response times\n",
    "\n",
    "### Performance Metrics:\n",
    "- Average total time: ~2-3 seconds per query\n",
    "- LLM generation: ~1-2 seconds\n",
    "- Retrieval: <1 second\n",
    "\n",
    "### Next Steps:\n",
    "The RAG pipeline is now ready for integration into the FastAPI backend!\n",
    "\n",
    "**Backend Development** will include:\n",
    "1. FastAPI endpoints for chat\n",
    "2. RAG engine module\n",
    "3. WebSocket support for streaming\n",
    "4. CORS configuration\n",
    "5. Error handling\n",
    "\n",
    "**Frontend Development** will include:\n",
    "1. React chat interface\n",
    "2. TanStack Query for API calls\n",
    "3. TanStack Table for source display\n",
    "4. Image viewer component\n",
    "5. Citation highlighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Book RAG)",
   "language": "python",
   "name": "ai-book-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
